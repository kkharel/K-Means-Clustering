# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
num_users = int(samples*2)
num_users
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) == conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
aggregated_data = sample_data.groupby('group').agg(
trials=('user_id', lambda x: x.nunique()),
successes=('converted', 'sum'),
)
# Display aggregated data
print(aggregated_data)
aggregated_data.T
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
group_assignments# Generate conversions
num_users = int(samples*2)
num_users
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
print(sample_data)
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
reticulate::repl_python()
import os
import pandas as pd
import numpy as np
os.chdir("C:/Users/kkhar/OneDrive/Desktop/K-Means-Clustering")
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)
xls = pd.ExcelFile("retail.xlsx")
sheet_names = xls.sheet_names
num_sheets = len(sheet_names)
print("Sheet Names:", sheet_names)
print("Number of Sheets:", num_sheets)
data1 = pd.read_excel("retail.xlsx", sheet_name = 0)
data2 = pd.read_excel("retail.xlsx", sheet_name = 1)
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked = stacked_df.dropna(subset = ['Customer ID'])
import pandasql as psql
query = """
SELECT Invoice, StockCode, Description, Quantity, InvoiceDate, Price, [Customer ID], Country
FROM stacked
GROUP BY Invoice, StockCode, Description, Quantity, InvoiceDate, Price, [Customer ID], Country
"""
stacked = psql.sqldf(query)
def starts_with_letter(string):
string = str(string)
return string[0].isalpha()
stacked = stacked[~stacked['Invoice'].apply(starts_with_letter)]
stacked.describe()
stacked = stacked[~(stacked['Price'] < 0)]
numeric_columns = stacked.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0
negative_mask.any()
stacked = stacked[~(stacked['Quantity'] < 0)]
stacked.columns.to_list()
# RFM Analysis of Customers
# Recency: If a customer made a purchase within last 3 months then we
# call them recent customers
stacked.head(n=2)
final_data = stacked.copy()
final_data.describe()
dataset_max = final_data['InvoiceDate'].max()
dataset_max = pd.to_datetime(dataset_max)
customer_max = final_data.groupby('Customer ID', as_index = False)['InvoiceDate'].max()
customer_max.columns = ['Customer ID', 'Latest_Invoice_Date']
customer_max['Latest_Invoice_Date'] = pd.to_datetime(customer_max['Latest_Invoice_Date'])
customer_max['Recency'] = customer_max.Latest_Invoice_Date.apply(lambda x: (dataset_max - x).days)
customer_max = customer_max.drop(['Latest_Invoice_Date'], axis = 1)
# Frequency, How Frequent Customers are making purchases
Invoice_Count = final_data.groupby(['Customer ID'], as_index = False).agg({'Invoice' : lambda x:len(x)})
Invoice_Count.columns = ['Customer ID', 'Frequency']
final_data['Sales'] = final_data['Price']*final_data['Quantity']
Total_Sales = final_data.groupby('Customer ID')['Sales'].sum().reset_index()
Total_Sales.columns = ['Customer ID', 'Monetary']
Total_Sales = Total_Sales.reset_index(drop = True)
customer_max = customer_max.reset_index(drop = True)
Invoice_Count = Invoice_Count.reset_index(drop = True)
merge1 = pd.merge(customer_max, Invoice_Count, on = 'Customer ID')
df = pd.merge(merge1, Total_Sales, on = 'Customer ID')
df.head(n=2)
Total_Sales[Total_Sales['Customer ID']==18102]
def calculate_rfm_scores(dataframe, r_col, f_col, m_col):
#dataframe.sort_values(by=[r_col, f_col, m_col], inplace=True)  # Sort the DataFrame by columns
r_quantiles = dataframe[r_col].quantile([0.2, 0.4, 0.6, 0.8])
f_quantiles = dataframe[f_col].quantile([0.2, 0.4, 0.6, 0.8])
m_quantiles = dataframe[m_col].quantile([0.2, 0.4, 0.6, 0.8])
def R_score(recency):
if recency <= r_quantiles.iloc[0]:
return 5
elif recency > r_quantiles.iloc[0] and recency <= r_quantiles.iloc[1]:
return 4
elif recency > r_quantiles.iloc[1] and recency <= r_quantiles.iloc[2]:
return 3
elif recency > r_quantiles.iloc[2] and recency <= r_quantiles.iloc[3]:
return 2
else:
return 1
def F_score(frequency):
if frequency <= f_quantiles.iloc[0]:
return 5
elif frequency > f_quantiles.iloc[0] and frequency <= f_quantiles.iloc[1]:
return 4
elif frequency > f_quantiles.iloc[1] and frequency <= f_quantiles.iloc[2]:
return 3
elif frequency > f_quantiles.iloc[2] and frequency <= f_quantiles.iloc[3]:
return 2
else:
return 1
def M_score(monetary):
if monetary <= m_quantiles.iloc[0]:
return 5
elif monetary > m_quantiles.iloc[0] and monetary <= m_quantiles.iloc[1]:
return 4
elif monetary > m_quantiles.iloc[1] and monetary <= m_quantiles.iloc[2]:
return 3
elif monetary > m_quantiles.iloc[2] and monetary <= m_quantiles.iloc[3]:
return 2
else:
return 1
dataframe['R'] = dataframe[r_col].apply(R_score)
dataframe['F'] = dataframe[f_col].apply(F_score)
dataframe['M'] = dataframe[m_col].apply(M_score)
return dataframe[['R', 'F', 'M']]
rfm_scores = calculate_rfm_scores(df, 'Recency', 'Frequency', 'Monetary')
df.head(n=2)
#df.tail()
#df[df['Customer ID'] == 18286]
#18286, #18287
# 1,4,2 and 4,2,1
#Customer ID Recency Frequency Monetary R F M Country
# do not merge, need to process Country in Lat Long - exclude for now
df['RFM'] = df['R'].astype(str)+df['F'].astype(str) + df['M'].astype(str)
df.head(n=2)
df['log_M'] = np.log(df['Monetary']+1)
def minMaxScaler(numArr):
minx = np.min(numArr)
maxx = np.max(numArr)
numArr = (numArr - minx) / (maxx - minx)
return numArr
df['scaled_Monetary'] = minMaxScaler(df['log_M'])
df['log_F'] = np.log(df['Frequency']+1)
df['scaled_Frequency'] = minMaxScaler(df['log_F'])
df['log_R'] = np.log(df['Recency']+1)
df['scaled_Recency'] = minMaxScaler(df['log_R'])
df['scaled_RFM'] = minMaxScaler(df['RFM'].astype(int))
def rfm_score_to_label(score):
rfm_mapping = {
555: "Champions",
554: "Champions",
553: "LoyalCustomers",
552: "PotentialLoyalist",
551: "PotentialLoyalist",
545: "Champions",
544: "Champions",
543: "LoyalCustomers",
542: "PotentialLoyalist",
541: "PotentialLoyalist",
535: "PotentialLoyalist",
534: "PotentialLoyalist",
533: "PotentialLoyalist",
532: "PotentialLoyalist",
531: "PotentialLoyalist",
525: "Promising",
524: "Promising",
523: "Promising",
522: "RecentCustomers",
521: "RecentCustomers",
515: "Promising",
514: "Promising",
513: "Promising",
512: "RecentCustomers",
511: "RecentCustomers",
455: "Champions",
454: "Champions",
453: "LoyalCustomers",
452: "PotentialLoyalist",
451: "PotentialLoyalist",
445: "Champions",
444: "Champions",
443: "LoyalCustomers",
442: "PotentialLoyalist",
441: "PotentialLoyalist",
435: "PotentialLoyalist",
434: "PotentialLoyalist",
433: "PotentialLoyalist",
432: "PotentialLoyalist",
431: "PotentialLoyalist",
425: "Promising",
424: "Promising",
423: "Promising",
422: "RecentCustomers",
421: "RecentCustomers",
415: "Promising",
414: "Promising",
413: "Promising",
412: "RecentCustomers",
411: "RecentCustomers",
355: "LoyalCustomers",
354: "LoyalCustomers",
353: "PotentialLoyalist",
352: "PotentialLoyalist",
351: "PotentialLoyalist",
345: "LoyalCustomers",
344: "LoyalCustomers",
343: "NeedAttention",
342: "NeedAttention",
341: "NeedAttention",
335: "NeedAttention",
334: "NeedAttention",
333: "NeedAttention",
332: "NeedAttention",
331: "AboutToSleep",
325: "NeedAttention",
324: "NeedAttention",
323: "NeedAttention",
322: "AboutToSleep",
321: "AboutToSleep",
315: "NeedAttention",
314: "NeedAttention",
313: "AboutToSleep",
312: "AboutToSleep",
311: "AboutToSleep",
255: "AtRisk",
254: "AtRisk",
253: "AtRisk",
252: "AboutToSleep",
251: "AboutToSleep",
245: "AtRisk",
244: "AtRisk",
243: "AtRisk",
242: "AboutToSleep",
241: "AboutToSleep",
235: "AtRisk",
234: "AtRisk",
233: "AtRisk",
232: "AboutToSleep",
231: "AboutToSleep",
225: "AtRisk",
224: "AtRisk",
223: "Hibernating",
222: "Hibernating",
221: "AboutToSleep",
215: "AtRisk",
214: "AtRisk",
213: "Hibernating",
212: "Hibernating",
211: "Hibernating",
155: "CannotLoseThem",
154: "CannotLoseThem",
153: "Hibernating",
152: "Hibernating",
151: "Lost",
145: "CannotLoseThem",
144: "CannotLoseThem",
143: "Hibernating",
142: "Hibernating",
141: "Lost",
135: "CannotLoseThem",
134: "CannotLoseThem",
133: "Hibernating",
132: "Hibernating",
131: "Lost",
125: "CannotLoseThem",
124: "CannotLoseThem",
123: "Hibernating",
122: "Lost",
121: "Lost",
115: "CannotLoseThem",
114: "CannotLoseThem",
113: "Lost",
112: "Lost",
111: "Lost",
}
return rfm_mapping.get(score, "Unknown")
backup = df.copy()
df['label'] = df['RFM'].astype(int).apply(rfm_score_to_label)
df['label'].unique()
df = pd.get_dummies(df, columns=['label'])
df[df.columns[df.columns.str.startswith('label_')]] = df[df.columns[df.columns.str.startswith('label_')]].astype(int)
df.head(n=2)
cols = [ 'Customer ID', 'scaled_Monetary', 'scaled_Frequency', 'scaled_Recency', 'label_AboutToSleep', 'label_AtRisk', 'label_CannotLoseThem', 'label_Champions', 'label_Hibernating', 'label_Lost', 'label_LoyalCustomers', 'label_NeedAttention', 'label_PotentialLoyalist', 'label_Promising', 'label_RecentCustomers']
cluster_data = df[cols].astype(float)
cluster_data.head()
cluster_data_b = cluster_data
cluster_data = cluster_data_b
cluster_data
cluster_data_dict = df.set_index('Customer ID').to_dict(orient = 'index')
first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
print(f'{key}: {value}')
faef
cluster_data_dict = cluster_data.set_index('Customer ID').to_dict(orient = 'index')
first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
print(f'{key}: {value}')
cluster_data_dict = cluster_data.set_index('Customer ID').to_dict(orient = 'records')
first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
print(f'{key}: {value}')
first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
print(f'{key}: {value}')
cluster_data_dict = cluster_data.set_index('Customer ID').to_dict(orient = 'records')
first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
cluster_data_dict
cluster_data_dict = cluster_data.set_index('Customer ID').to_dict(orient = 'records')
cluster_data_dict
cluster_data_dict.head(n=2)
cluster_data_dict = cluster_data.set_index('Customer ID').apply(lambda x: x.values.tolist(), axis=1).to_dict()
first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
print(f'{key}: {value}')
first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
print(f'{key}: {value}')
class Centroid:
def __init__(self, location):
self.location = location
self.closest_users = set()
initial_centroids = random.sample(sorted(list(user_feature_map.keys())), k)
initial_centroids = random.sample(sorted(list(cluster_data_dict.keys())), k)
import random
initial_centroids = random.sample(sorted(list(cluster_data_dict.keys())), k)
k = 3
initial_centroids = random.sample(sorted(list(cluster_data_dict.keys())), k)
initial_centroids
centroids = {f'cluster{ik}Centroids': cluster_data_dict[initial_centroids[ik]] for ik in range(k)}
centroids
centroids = {f'cluster{ik}Centroids': cluster_data_dict[initial_centroids_customers[ik]] for ik in range(k)}
initial_centroids_customers = random.sample(sorted(list(cluster_data_dict.keys())), k)
initial_centroids_customers
centroids = {f'cluster{ik}Centroids': cluster_data_dict[initial_centroids_customers[ik]] for ik in range(k)}
centroids
clusters = {f'cluster{ik}Users': [] for ik in range(k)}
clusters
clusters = {f'cluster{ik}CustomerID': [] for ik in range(k)} # initialize empty list to store Customer IDs that falls into each clusters
clusters
distance = {f'Centroid{ik}distance': u: sum([centroids[f'cluster{ik}Centroids'][j] - cluster_data_dict[u][j] for j in range(num_features_per_user)]) for u in user_feature_map} for ik in range(k)}
distance = {f'Centroid{ik}distance': {u: sum([centroids[f'cluster{ik}Centroids'][j] - cluster_data_dict[u][j] for j in range(num_features_per_user)]) for u in user_feature_map} for ik in range(k)}
distance = {f'Centroid{ik}distance': {u: sum([centroids[f'cluster{ik}Centroids'][j] - cluster_data_dict[u][j] for j in range(num_features_per_user)]) for u in cluster_data_dict} for ik in range(k)}
num_features_per_user = 14
centroids
num_features_per_user = 14
distance = {f'Centroid{ik}distance': {u: sum([centroids[f'cluster{ik}Centroids'][j] - cluster_data_dict[u][j] for j in range(num_features_per_user)]) for u in cluster_data_dict} for ik in range(k)}
distance
first_5_records = {k: distance[k] for k in list(distance)[:5]}
for key, value in first_5_records.items():
print(f'{key}: {value}')
first_5_records = {k: distance[k] for k in list(distance)[:1]}
for key, value in first_5_records.items():
print(f'{key}: {value}')
distance = {}
for ik in range(k): # Loop over centroids
centroid_distances = {} # intialize empty dict to save centroid distances
for u in cluster_data_dict: # Loop over data points
total_distance = 0 # initialize the distance to zero for this data point
for j in range(num_features_per_user): # Loop over features
total_distance += centroids[f'cluster{ik}Centroids'][j] - cluster_data_dict[u][j] # Calculate the distance along each feature dimension and add it to tota_distance
centroid_distances[u] = total_distance
distance[f'Centroid{ik}distance'] = centroid_distances
distance
for key,value in distance.items():
print{f"{key}: {value}"}
for key,value in distance.items():
print(f"{key}: {value}")
count = 0
for key, value in distance.items():
print(f"{key}: {value}")
count += 1
if count >= 3:
break
count = 0
for key, value in distance.items():
print(f"{key}: {value}")
count += 1
if count >= 1:
break
da
