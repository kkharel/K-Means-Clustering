alpha = 0.05  # Significance level (5%)
beta = 0.2  # Power (80%)
p_control = 0.10  # Estimated proportion in the control group (10%)
samples = calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='two_sided')
samples
def calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='one_sided'):
if mode not in ['one_sided', 'two_sided']:
raise ValueError('The modes are one_sided and two_sided')
Z_alpha_over_2 = sp_stats.norm.ppf(1 - alpha / 2)
Z_beta = sp_stats.norm.ppf(1 - beta)
pooled_variance = p_control * (1 - p_control)
if mode == 'two_sided':
n = (2 * (Z_alpha_over_2 + Z_beta)**2 * pooled_variance) / MDE**2
else:
n = (Z_alpha_over_2**2 * pooled_variance) / MDE**2
return int(np.ceil(n))
MDE = 0.02  # Minimum Detectable Effect (3% improvement)
alpha = 0.05  # Significance level (5%)
beta = 0.2  # Power (80%)
p_control = 0.10  # Estimated proportion in the control group (10%)
samples = calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='two_sided')
samples
sample_data = generate_sample_data(required_sample_size = sample, p_control, MDE)
num_users = int(required_sample_size * 2)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
sample_data
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
samples = calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='two_sided')
samples
num_users = int(required_sample_size * 2)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
sample_data
samples
num_users = int(required_sample_size * 2)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
print(sample_data)
aggregated_data = sample_data.groupby('group').agg(
trials=('user_id', lambda x: x.nunique()),
successes=('converted', 'sum'),
)
print(aggregated_data)
aggregated_data.T
num_users = int(required_sample_size)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
sample_data
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
samples
num_users = int(required_sample_size)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) = conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
num_users = int(required_sample_size)
num_users
num_users = int(samples)
num_users
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
num_users = int(samples*2)
num_users
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) == conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
aggregated_data = sample_data.groupby('group').agg(
trials=('user_id', lambda x: x.nunique()),
successes=('converted', 'sum'),
)
# Display aggregated data
print(aggregated_data)
aggregated_data.T
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
group_assignments# Generate conversions
num_users = int(samples*2)
num_users
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
print(sample_data)
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
reticulate::repl_python()
import os
import pandas as pd
import numpy as np
os.chdir("C:/Users/kkhar/OneDrive/Desktop/K-Means-Clustering")
pd.set_option("display.max_columns", None)
xls = pd.ExcelFile("retail.xlsx")
sheet_names = xls.sheet_names
num_sheets = len(sheet_names)
print("Sheet Names:", sheet_names)
print("Number of Sheets:", num_sheets)
data1 = pd.read_excel("retail.xlsx", sheet_name = 0)
data2 = pd.read_excel("retail.xlsx", sheet_name = 1)
data1.columns
data2.columns
data1.info()
data2.info()
stacked_df = pd.concat([data1, data2], ignore_index = True)
null_mask = stacked_df.isnull().any(axis = 1)
null_rows = stacked_df[null_mask]
print(null_rows)
stacked = stacked_df.dropna(subset = ['Customer ID'])
stacked.isnull().sum()
# Removing duplicate entries from a table
#import pandasql as psql
#query = """
#    SELECT *
#    FROM stacked
#    GROUP BY Invoice, StockCode, Description, Quantity, InvoiceDate, Price, [Customer ID], Country
#"""
#stacked = psql.sqldf(query)
def starts_with_letter(string):
string = str(string)
return string[0].isalpha()
stacked = stacked[~stacked['Invoice'].apply(starts_with_letter)]
stacked.describe()
numeric_columns = stacked.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0
negative_mask.any()
stacked[stacked['Quantity'] < 0]
stacked.columns.to_list()
# RFM Analysis of Customers
# Recency: If a customer made a purchase within last 3 months then we
# call them recent customers
stacked.head(n=2)
final_data = stacked.copy()
dataset_max = final_data['InvoiceDate'].max()
dataset_max = pd.to_datetime(dataset_max)
customer_max = final_data.groupby('Customer ID', as_index = False)['InvoiceDate'].max()
customer_max.columns = ['Customer ID', 'Latest_Invoice_Date']
customer_max['Latest_Invoice_Date'] = pd.to_datetime(customer_max['Latest_Invoice_Date'])
customer_max['Recency'] = customer_max.Latest_Invoice_Date.apply(lambda x: (dataset_max - x).days)
customer_max = customer_max.drop(['Latest_Invoice_Date'], axis = 1)
# Frequency, How Frequent Customers are making purchases
Invoice_Count = final_data.groupby('Customer ID', as_index = False)['Invoice'].nunique()
Invoice_Count.columns = ['Customer ID', 'Frequency']
Invoice_Count
# Monetary, How much did Customers Spend
final_data.head()
final_data['Sales'] = final_data['Price']*final_data['Quantity']
final_data
Total_Sales = final_data.groupby('Customer ID')['Sales'].sum().reset_index()
Total_Sales.columns = ['Customer ID', 'Monetary']
Total_Sales
merge1 = pd.merge(customer_max, Invoice_Count, on = 'Customer ID')
df = pd.merge(merge1, Total_Sales, on = 'Customer ID')
def calculate_rfm_scores(dataframe, r_col, f_col, m_col):
#dataframe.sort_values(by=[r_col, f_col, m_col], inplace=True)  # Sort the DataFrame by columns
r_quantiles = dataframe[r_col].quantile([0.2, 0.4, 0.6, 0.8])
f_quantiles = dataframe[f_col].quantile([0.2, 0.4, 0.6, 0.8])
m_quantiles = dataframe[m_col].quantile([0.2, 0.4, 0.6, 0.8])
def R_score(recency):
if recency <= r_quantiles.iloc[0]:
return 5
elif recency > r_quantiles.iloc[0] and recency <= r_quantiles.iloc[1]:
return 4
elif recency > r_quantiles.iloc[1] and recency <= r_quantiles.iloc[2]:
return 3
elif recency > r_quantiles.iloc[2] and recency <= r_quantiles.iloc[3]:
return 2
else:
return 1
def F_score(frequency):
if frequency <= f_quantiles.iloc[0]:
return 5
elif frequency > f_quantiles.iloc[0] and frequency <= f_quantiles.iloc[1]:
return 4
elif frequency > f_quantiles.iloc[1] and frequency <= f_quantiles.iloc[2]:
return 3
elif frequency > f_quantiles.iloc[2] and frequency <= f_quantiles.iloc[3]:
return 2
else:
return 1
def M_score(monetary):
if monetary <= m_quantiles.iloc[0]:
return 5
elif monetary > m_quantiles.iloc[0] and monetary <= m_quantiles.iloc[1]:
return 4
elif monetary > m_quantiles.iloc[1] and monetary <= m_quantiles.iloc[2]:
return 3
elif monetary > m_quantiles.iloc[2] and monetary <= m_quantiles.iloc[3]:
return 2
else:
return 1
dataframe['R'] = dataframe[r_col].apply(R_score)
dataframe['F'] = dataframe[f_col].apply(F_score)
dataframe['M'] = dataframe[m_col].apply(M_score)
return dataframe[['R', 'F', 'M']]
rfm_scores = calculate_rfm_scores(df, 'Recency', 'Frequency', 'Monetary')
#df.tail()
#df[df['Customer ID'] == 18286]
#18286, #18287
# 1,4,2 and 4,2,1
df
df.groupby('Countries')['Customer ID']
stacked.groupby('Countries')['Customer ID']
stacked.columns.to_list()
stacked.groupby('Country')['Customer ID']
x = stacked.groupby('Country')['Customer ID']
x
x.head()
import pandasql as psql
query = """
SELECT [Customer ID], Country
FROM stacked
GROUP BY [Customer ID], Country
"""
countries = psql.sqldf(query)
countries
df
x= 1012/2
y = x**0
str(y)
40/5
