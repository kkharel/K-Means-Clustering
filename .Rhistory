start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) = conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
num_users = int(required_sample_size)
num_users
num_users = int(samples)
num_users
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
num_users = int(samples*2)
num_users
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) == conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
aggregated_data = sample_data.groupby('group').agg(
trials=('user_id', lambda x: x.nunique()),
successes=('converted', 'sum'),
)
# Display aggregated data
print(aggregated_data)
aggregated_data.T
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
group_assignments# Generate conversions
num_users = int(samples*2)
num_users
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
print(sample_data)
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
reticulate::repl_python()
data = pd.read_csv("retail.csv")
import pandas as pd
data = pd.read_csv("retail.csv")
getcwd()
import os
os.getcwd()
os.chdir("C:/Users/kkhar/OneDrive/Documents/K-Means-Clustering")
data = pd.read_csv("retail.csv")
os.getcwd()
data = pd.read_xlsx("retail.xlsx")
data = pd.read_csv("retail.csv")
data = pd.read_csv("retail")
data = pd.read_csv("retail.csv")
os.chdir("C:/Users/kkhar/OneDrive/Documents")
data = pd.read_csv("retail.csv")
os.getcwd()
os.chdir("C:/Users/kkhar/OneDrive/Documents/K-Means-Clustering")
os.chdir("C:/Users/kkhar/OneDrive/Desktop/K-Means-Clustering")
data = pd.read_csv("retail.csv")
import xlsx
data = pd.read_excel("retail.xlsx")
data
data.head(n=2)
data = pd.read_excel("retail.xlsx")
data.head(n=2)
data.describe()
data = pd.read_excel("retail.xlsx", sheet_name = 0)
sheet_names = xls.sheet_names
xls = pd.ExcelFile("retail.xlsx")
sheet_names = xls.sheet_names
num_sheets = len(sheet_names)
print("Sheet Names:", sheet_names)
print("Number of Sheets:", num_sheets)
data1.describe()
data1 = pd.read_excel("retail.xlsx", sheet_name = 0)
data2 = pd.read_excel("retail.xlsx", sheet_name = 1)
data1.describe()
data1.colnames
data1.colnames()
data1.columns
data2.columns
data2.describe()
data1.info()
data2.infor()
data2.info()
stackes_df = pd.concat([data1, data2], ignore_index = True)
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked_df.head(n=2)
stacked_df.describe()
stacked_df.isnull().sum()
stacked_df['Description']
stacked_df['Description'].value_counts()
stacked_df['Description'].unique().value_counts()
stacked_df['Description'].nunique().value_counts()
stacked_df['Description'].nunique()
stacked_df.isnull().sum()
stacked_df
pd.set_options("display.max_columns", None)
stacked_df
pd.set_options("display.max_columns", None)
pd.set_option("display.max_columns", None)
stacked_df
stacked_df.isnull().sum()
stacked_df.describe()
data1.isnull().sum()
data2.isnull().sum()
final_data = stacked_df.dropna(subset = ['Customer ID'])
final_data.describe()
final_data.info()
final_data.isnull().sum()
final_data.columns
final_data.columns.to_list()
final_data.info()
final_data.head(n=2)
data['InvoiceDate'].max()
data['InvoiceDate'].min()
data['InvoiceDate'].max() - 30
data['InvoiceDate'].max() - data['InvoiceDate'].max()*30
data['InvoiceDate'].max()
df.groupby('Customer ID')['InvoiceDate'].max()
final_data['InvoiceDate'].max()
final_data['InvoiceDate'].min()
final_data.groupby('Customer ID')['InvoiceDate'].max()
(final_data['InvoiceDate'].max() - final_data.groupby('Customer ID')['InvoiceDate'].max()).dt.days
final_data['Recency'] = (final_data['InvoiceDate'].max() - final_data.groupby('Customer ID')['InvoiceDate'].max()).dt.days
final_data
final_data.isnull().sum()
final_data = stacked_df.dropna(subset = ['Customer ID'])
final_data.isnull().sum()
final_data.columns.to_list()
final_data.info()
(final_data['InvoiceDate'].max() - final_data.groupby('Customer ID')['InvoiceDate'].max()).dt.days
final_data.columns.to_list()
final_data.head(n=2)
data.groupby('Customer ID')['Invoice'].nunique()
data.groupby('Customer ID')['Invoice'].nunique().max()
data.groupby('Customer ID')['Invoice'].nunique().min()
final_data.groupby('Customer ID')['Invoice'].nunique()
data['Price']*data['Quantity']
final_data['Sales'] = final_data['Price']*final_data['Quantity']
final_data
final_data.isnull().sum()
final_data['Sales'].max()
final_data['Sales'].min()
final_data['Sales'] = -168469.6
final_data.groupby('Customer ID')['Invoice'].nunique()
final_data['Sales'] = final_data['Price']*final_data['Quantity']
final_data['Sales'] == -168469.6
final_data[['Sales'] < 0]
final_data['Sales' < 0]
final_data[final_data['Sales' < 0]]
final_data[final_data['Sales'] < 0]
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked_df.isnull().sum()
stacked_df
final_data = stacked_df.dropna(subset = ['Customer ID'])
final_data.isnull().sum()
final_data[final_data['Price'] < 0]
final_data[final_data['Quantity'] < 0]
final_data[final_data < 0]
negative_mask = final_data < 0
numeric_columns = df.select_dtypes(include=['number'])
numeric_columns = final_data.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0
negative_mask.any()
final_data.columns.to_list()
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked_df.isnull().sum()
data1.isnull().sum()
data2.isnull().sum()
final_data = stacked_df.dropna(subset = ['Customer ID'])
final_data.isnull().sum()
final_data.columns.to_list()
numeric_columns = final_data.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0
negative_mask.any()
final_data[final_data['Quantity'] < 0]
final_data.columns.to_list()
final_data.groupby('Customer ID', 'StockCode')['Quantity'].sum().reset_index()
final_data.groupby(['Customer ID', 'StockCode'])['Quantity'].sum().reset_index()
final_data.groupby(['Invoice', 'Customer ID', 'StockCode'])['Quantity'].sum().reset_index()
final_data[final_data['Quantity'] < 0]
final_data.columns.to_list()
final_data.groupby(['Invoice', 'StockCode', 'Description', 'InvoiceDate', 'Price', 'Customer ID', 'Country'])['Quantity'].sum()
final_data.groupby(['Invoice', 'StockCode', 'Description', 'InvoiceDate', 'Price', 'Customer ID', 'Country'])['Quantity'].sum().reset_index()
grouped_df = final_data.groupby(['Customer ID', 'StockCode'])['Quantity'].sum().reset_index()
grouped_df
returns_df = grouped_df.copy()
returns_df['Quantity'] = -returns_df['Quantity']
returns_df
result_df = pd.merge(grouped_df, returns_df, on=['Customer ID', 'StockCode'])
result_df.columns = ['Customer ID', 'StockCode', 'Bought', 'Returned']
result_df['Net'] = result_df['Bought'] + result_df['Returned']
print(result_df)
final_data = stacked_df.dropna(subset = ['Customer ID'])
final_data.isnull().sum()
numeric_columns = final_data.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0
negative_mask.any()
final_data[final_data['Quantity'] < 0]
final_data[final_data['Invoice'] = C489449]
final_data[final_data['Invoice'] == C489449]
final_data[final_data['Invoice'] == 'C489449']
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked_df.isnull().sum()
numeric_columns = final_data.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0
negative_mask.any()
final_data[final_data['Quantity'] < 0]
final_data[final_data['Invoice'] == 'C489449']
final_data[final_data['Quantity'] < 0]
final_data[final_data['Invoice'] == 'C581569']
final_data['Customer ID'].null()
final_data['Customer ID'].isnull()
final_data['Customer ID'].isnull().sum()
final_data[final_data['Customer ID'].isnull()]
final_data[final_data['Customer ID'] ==  'NaN']
final_data[final_data['Customer ID'] ==  '']
final_data[final_data['Customer ID'] ==  ' ']
null_mask = final_data.isnull().any(axis = 1)
null_rows = df[null_mask]
null_rows = final_data[null_mask]
print(null_rows)
final_data = stacked_df.dropna(subset = ['Customer ID'])
final_data.isnull().sum()
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked_df.isnull().sum()
null_mask = final_data.isnull().any(axis = 1)
null_rows = final_data[null_mask]
print(null_rows)
final_data.isnull().any()
final_data.isnull().any(axis = 1)
final_data.isnull().sum()
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked_df.isnull().sum()
null_mask = stacked_df.isnull().any(axis = 1)
null_rows = stacked_df[null_mask]
print(null_rows)
stacked_df[stacked_df['Invoice'] = '489464']
stacked_df[stacked_df['Invoice'] == '489464']
stacked_df[stacked_df['Invoice'] == 489464]
print(null_rows)
stacked_df[stacked_df['Invoice'] == 1067001]
stacked_df[stacked_df['Invoice'] == 581498]
stacked_df[stacked_df['Quantity'] < 0]
stacked_df[stacked_df['Invoice'] = 'C489449']
stacked_df[stacked_df['Invoice'] == 'C489449']
stacked_df[stacked_df['Quantity'] < 0 & stacked_df['Quantity'] > 0]
stacked_df[stacked_df['Quantity'] > 0]
stacked_df[stacked_df['Quantity'] < 0]
stacked_df[stacked_df['Invoice'] == 'C581569']
stacked_df[stacked_df['StockCode'] == 20979]
stacked_df[stacked_df['StockCode'] == 20979 & stacked_df['Invoice'] = 'C581409']
stacked_df[stacked_df['StockCode'] == 20979 & stacked_df['Invoice'] == 'C581409']
stacked_df([stacked_df['StockCode'] == 20979 & stacked_df['Invoice'] == 'C581409'])
stacked_df([stacked_df['StockCode'] == 20979 & stacked_df['Invoice'] == C581409])
stacked_df([stacked_df['StockCode'] == 20979])
stacked_df[stacked_df['StockCode'] == 20979]
stacked_df[stacked_df['Invoice'] == 'C581569']
stacked_df[stacked_df['Invoice'] == '581569']
final_data.columns.to_list()
is_cancellation = df['Invoice'].str.startswith('C')
cancellations = df[is_cancellation]
is_cancellation = stacked_df['Invoice'].str.startswith('C')
cancellations = stacked_df[is_cancellation]
original_invoice_numbers = cancellations['Invoice'].str.lstrip('C')
cancellations = stacked_df[is_cancellation]
is_cancellation = stacked_df['Invoice'].str.startswith('C')
cancellations = stacked_df[is_cancellation]
stacked_df = pd.concat([data1, data2], ignore_index = True)
null_mask = stacked_df.isnull().any(axis = 1)
null_rows = stacked_df[null_mask]
print(null_rows)
stacked_df[stacked_df['Quantity'] < 0]
is_cancellation = stacked_df['Invoice'].str.startswith('C')
cancellations = stacked_df[is_cancellation]
is_cancellation
is_cancellation = stacked_df['Invoice'].str.startswith('c')
is_cancellation
is_cancellation.dropna()
is_cancellation = is_cancellation.dropna()
is_cancellation
is_cancellation == True
is_cancellation[[is_cancellation == True]]
is_cancellation = df['Invoice'].str.startswith('C')
is_cancellation = stacked_df['Invoice'].str.startswith('C')
cancellations = df[is_cancellation]
cancellations = stacked_df[is_cancellation]
stacked_df[stacked_df['Invoice'] == '581569']
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked_df[stacked_df['Quantity'] < 0]
is_cancellation = stacked_df['Invoice'].str.startswith('C')
is_cancellation
stacked_df['Invoice'].str.startswith('C')
stacked_df['Invoice'].str.startswith('C').dropna()
is_cancellation = stacked_df['Invoice'].str.startswith('C').dropna()
cancellations = stacked_df[is_cancellation]
cancellations
cancellations = stacked_df[is_cancellation]
cancellations
is_cancellation = stacked_df['Invoice'].str.startswith('C').dropna(0)
is_cancellation = stacked_df['Invoice'].str.startswith('C')
cancellations = stacked_df[is_cancellation]
is_cancellation = stacked_df['Invoice'].str.startswith('C').replace('NaN', 0)
cancellations = stacked_df[is_cancellation]
is_cancellation = stacked_df['Invoice'].str.startswith('C')
original_transactions = df[df['Invoice'].isin(original_invoice_numbers)]
stacked_df[stacked_df['Invoice'].str.startswith('C')]
stacked_df['Invoice'].str.startswith('C')
is_cancellation = stacked_df['Invoice'].str.startswith('C')
cancellations = stacked_df[is_cancellation]
stacked_df = stacked_df.dropna(subset=['Invoice'])
is_cancellation = stacked_df['Invoice'].str.startswith('C')
cancellations = stacked_df[is_cancellation]
stacked_df = stacked_df.dropna(subset=['Invoice'])
is_cancellation = stacked_df['Invoice'].str.startswith('C')
is_cancellation
stacked_df = stacked_df.dropna(subset=['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country'], inplace=True)
is_cancellation = stacked_df['Invoice'].str.startswith('C')
stacked_df = stacked_df.dropna(subset=['Invoice', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'Price', 'Customer ID', 'Country'], inplace=True)
is_cancellation = stacked_df['Invoice'].str.startswith('C')
stacked_df = pd.concat([data1, data2], ignore_index = True)
stacked_df = stacked_df.dropna(subset = ['Customer ID'])
is_cancellation = stacked_df['Invoice'].str.startswith('C')
cancellations = stacked_df[is_cancellation]
is_cancellation
stacked_df[stacked_df['Invoice'].str.startswith('C')]
stacked_df = pd.concat([data1, data2], ignore_index = True)
null_mask = stacked_df.isnull().any(axis = 1)
null_rows = stacked_df[null_mask]
print(null_rows)
stacked_df = stacked_df.dropna(subset = ['Customer ID'])
stacked_df.isnull().sum()
stacked_df[stacked_df['Invoice'].str.startswith('C')]
stacked = stacked_df.dropna(subset = ['Customer ID'])
stacked[stacked['Invoice'].str.startswith('C')]
stacked['Invoice'].str.startswith('C')
stacked['Invoice'].str.startswith('C').dropna()
stacked_df[stacked['Invoice'].str.startswith('C').dropna()]
stacked_df[stacked['Invoice'].str.startswith('C').dropna()].reset_index()
stacked['Invoice'].str.startswith('C').dropna()
stacked['Invoice'].str.startswith('C')
x = stacked['Invoice'].str.startswith('C')
stacked[stacked[x] == True]
stacked[stacked[x.dropna()] == True]
x.dropna()
k = x.dropna()
stacked[stacked[k] == True]
stacked[stacked[k]]
stacked[k]
