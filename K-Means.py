# K- Means Clustering

# Use Cases ex - Market Segmentation, Social Network Analysis, Search result groupings
# Medical imaging, image segmentation, anomaly detection, Recommendation systems etc...

# K means complexity O(n)

# Types of Clustering
  # Centroid Based Clustering
  # Density Based Clustering
  # Distribution Based Clustering
  # Hierarchical Clustering
  
# Steps to cluster data:
  # Prepare data
  # Create similarity Metric
  # Run Clustering Algorithm
  # Interpret results and adjust clustering
  
# Prepare Data:
  # Normalizing Data: well-suited for processing common distributions like Gaussian. 
  # also when data set lacks enough data to create quantiles
  
  # Log Transform: when dataset conforms power law distribution that clumps data at the low end
  # We process a power-law distribution by using log transform
  
  # Quantiles: When data does not conform Gaussian or power-law distribution. General approach
  # that applies to any data distribution
  # Divide the data into intervals where each interval contains equal number of examples. These
  # example boundaries are called quantiles
  # Steps to convert data into quantiles:
    # Decide the number of intervals
    # Define intervals such that each interval has equal number of examples
    # Replace each example by the index of the interval it falls in
    # Bring the indexes to same range as other feature data by scaling the index values to [0,1]
    
  # Quantiles are the best default choice to transform data. Rule of thumb: To create n
  # quantiles, we should have at least 10*n examples. 
  
# Create similarity metric:
  # Manual Similarity Measure:
  # When features are numeric, can combine them into a single number.
  # When features are categorical - Single Valued (univalent), Multi Value (multivalent). 
  # Single valued are easier, when it is Multi Valued then use ratio of common values aka
  # Jaccard similarity
  # To calculate the overall similarity between pairs by combining per-feature similarity
  # using root mean squared error.

  # Supervised Similarity Measure:
  # Instead of comparing manually-combined feature data, we can reduce the feature data
  # to representations called embeddings and then compare the embeddings.
  # Embeddings are generated by training a supervised deep neural network (DNN) on the
  # feature data itself. 
  # Embeddings map the feature data to a vector in embedding space
  
  # When to use manual vs supervised measures:
    # Eliminate redundant information in correlated features:
      # In manual, we need to separately investigate correlations between features, No.
      # In supervised, DNN eliminates redudant information, Yes.
      
    # Provide insight into calculated similaries:
      # In manual ,Yes. 
      # In supervised, embeddings cannot be deciphered, No.
      
    # Suitable for small datasets with few features:
      # In manual, designing a manual measure with few features is easy.
      # In supervised, small datasets do not provide enough training data for DNN.
      
    # Suitable for large datasets with many features:
      # In manual, manually eliminating redundant information from multiple features
      # and then combining them is very difficult, No.
      # In supervised, the DNN automatically eliminates redundant information and combines
      # features, yes.
      
    # Steps for Supervised similarity measure:
      # Input feature data
      # Choose DNN: auto encoder, predictor
      # Extract Embeddings
      # Choose Measurement: Dot Product, Cosine, Euclidean distance
      
    # Choose DNN based on Training Labels:
      # Reduce the feature data to embeddings by training a DNN that uses the same feature
      # data both as input and as the labels
      # Example, use age, bmi, glucose to predict those features themselves
      # In order to use the feature daat to predict the same feature data, the DNN is forced
      # to reduce the input feature data to embeddings. Use these embeddings to calculate similarity.
      # DNN that learns embeddings of input data by predicting the input data itself is called
      # autoencoder. Because an autoencoder's hidden layers are smaller than the input and output
      # layers, the autoencoder is forced to learn a compressed representation of the input feature
      # data. Once the DNN is trained, we extract the embeddings from the last hidden layer to calculate
      # similarity.
      
      # Autoencoder is simplest choice to fenerate embeddings. 
      # Autoencoder is not optimal choice when certain features could be more important than others in determining similarity.
      # Assume age is more important than bmi for stroke prediction. In such cases, use only
      # the important feature as the training label for the DNN. Since this DNN predicts a specific
      # input feature instead of predicting all input features, it is called a predictor DNN
      
      # Guidelines to Choose a feature as the label:
      # Prefer numeric features to categorical features as labels because loss is easier to 
      # calculate and interpret for numeric features
      # Do not use categorical features with cardinality <= 100 as labels. If we do, DNN will not
      # be forced to reduct input data to embeddings because a DNN can easily predict low-cardinality
      # categorical labels.
      # Remove the feature that we use as the label from the input to DNN else DNN will
      # perfectly predict the output
      
      # Depending on the choice of labels, DNN is either autoencoder DNN or predictor DNN
      
    # Loss Function for DNN
      # Calculate the loss for every output of the DNN.
        # For numeric, use mean squared error
        # For univalent categorical, use log loss
        # For multivalent categorical, use softmax cross entropy loss
      # Calculate total loss by summing the loss for every output.
      
    # Note to ensure that each feature contributes proportionally to the loss. Ex: if we 
    # convert color data to RGB values, then we have three outputs. Summing the loss for
    # three output means weighting three times heavily than other features. Instead, multiply by 1/3
    
    # Using DNN in an Online System:
      # Online ML system has continuous stream of new input data. We need to train DNN on 
      # new data. However, if we retrain the DNN from scratch, then our embeddings will be 
      # different because DNNs are initialized with random weights. Instead, we warm-start
      # the DNN with the existing weights and then update the DNN with new data
    
    
    # Generating Embeddings:
    
      # Imagine we have a housing dataset: PRice, Size, Postal Code, Number of Bedrooms, Type of House, Garage, Colors
      
      # Now Pre process the data
        # Price - Poisson Distribution - Quantize and scale to [0,1]
        # Size - Poisson Distribution - Quantize and scale to [0,1]
        # Postal Code - Categorical - Convert to Longitude and Latitude, Quantize and scale to [0,1]
        # Number of bedrooms - Integer - Clip Outliers and scale to [0,1]
        # Type of House - Catgorical - Convert to One-Hot Encoding
        # Garage - 0 or 1 - Leave as is
        # Colors - Categorical - Convert to RGB values and process as numeric data
        
      # Now choose a predictor ot auto encoder:
        # To generate embeddings, we can choose either autoencoder or a predictor. Default
        # choice is autoencoder. We choose predictor instead if specific features in our dataset
        # deetermine similarity. We will look at both cases
        
        # Train a predictor:
          # We need to choose the features as training labels for DNN that are important in
          # determining similarity between examples. Assume price is most important in determining similarity between houses
          
          # Choose a price as the training label, and remove it from the input feature data to
          # the DNN. Train the DNN by using all other features as input data. For training, the 
          # loss function is simply the MSE between predicted and actual price
          
        # Train a Autoencoder:
          # Ensure the hidden layers of the autoencoder are smaller than the input and output layers
          # Calculate the loss for each output 
          # Create the loss function by summing the losses for each output. Ensure the weightings
          # Train the DNN
          
      # Now Extracting Embeddings from the DNN:
        # After training the DNN, whether predictor or autoencoder, extract the embeddings
        # for an example from the DNN. Extract the embedding by using feature data of the 
        # example as input, and read the outputs of the final hidden layer. These outputs
        # form the embedding vector. 
        
      # Now we Measure Similarity from Embeddings
        # We have three choices: Euclidean distance, Cosine and Dot Product. When we have
        # examples that appear frequently in the training set ( ex: popular youtube videos)
        # then they tend to have embedding vectors with large lengths. If we want to capture
        # popularity then we choose dot product. However, the risk is that popular examples
        # may skew the similarity metric. To balance this skew, we can raise the length to
        # an exponent a > 1 to calcuate the dot product
        
    # How to check quality of clustering?
      # Commonly used metrics: Cluster Cardinality, Cluster Magnitude, Performance of Downstream system
      
    # Cluster Cardinality: is the number of examples per cluster. Plot the cluster cardinality,
    # for all clusters and investigate for outliers. Cluster vs Number of Points in Cluster
    
    # Cluster Magnitude: is the sum of distances from all examples to the centroid of the 
    # cluster.Check how the magnitude varies across clusters and investiage anomalies.
    # Cluster vs total point-to-centroid distance
    
    # Magnitude vs Cardinality: Higher cluster cardinality tends to have higer magnitude. Clusters
    # are anomalous when cardinality does not correlate with magnitude relative to other 
    # clusters. Plot Magnitude aganist Cardinality for inspection
    
    
    # Performance of the Similarity Measure:
    
    # Simple Check: identify pairs of examples that are more or less similar than other pairs
    # then calculate the similarity measure for each pair of examples and ensure that the 
    # similarity measure for more similar examples is higher than the similiarity measure for
    # less similar examples.
    
    
    # Optimum Number of Clusters: Plot the Number of Clusters against Total point-to-centroid
    # distance. When loss becomes marginal with increasing k, we can approximate the k value.

import os
import pandas as pd
os.chdir("C:/Users/kkhar/OneDrive/Desktop/K-Means-Clustering")

pd.set_option("display.max_columns", None)

xls = pd.ExcelFile("retail.xlsx")
sheet_names = xls.sheet_names
num_sheets = len(sheet_names)
print("Sheet Names:", sheet_names)
print("Number of Sheets:", num_sheets)

data1 = pd.read_excel("retail.xlsx", sheet_name = 0)
data2 = pd.read_excel("retail.xlsx", sheet_name = 1)

data1.describe()
data1.columns
data2.columns
data2.describe()

data1.info()
data2.info()

stacked_df = pd.concat([data1, data2], ignore_index = True)

null_mask = stacked_df.isnull().any(axis = 1)
null_rows = stacked_df[null_mask]
print(null_rows)

stacked = stacked_df.dropna(subset = ['Customer ID'])
stacked_df.isnull().sum()

stacked_df[stacked_df['Quantity'] < 0]

stacked_df[stacked_df['StockCode'] == 20979]
stacked_df[stacked_df['Invoice'] == '581569']

x = stacked['Invoice'].str.startswith('C')





final_data = stacked_df.dropna(subset = ['Customer ID'])
final_data.isnull().sum()

numeric_columns = final_data.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0

negative_mask.any()
final_data[final_data['Quantity'] < 0]

final_data.columns.to_list()



null_mask = final_data.isnull().any(axis = 1)
final_data.isnull().any(axis = 1)
null_rows = final_data[null_mask]
print(null_rows)
final_data[final_data['Customer ID'] ==  ]
# Returns Handling


# RFM Analysis of Customers 

# Recency: If a customer made a purchase within last 3 months then we 
# call them recent customers
final_data.head(n=2)

final_data['InvoiceDate'].max()
final_data['InvoiceDate'].min()

final_data.groupby('Customer ID')['InvoiceDate'].max()

(final_data['InvoiceDate'].max() - final_data.groupby('Customer ID')['InvoiceDate'].max()).dt.days

# Frequency, How Frequent Customers are making purchases
final_data.groupby('Customer ID')['Invoice'].nunique()

# Monetary Value
final_data['Sales'] = final_data['Price']*final_data['Quantity']



