# K- Means Clustering

# Use Cases ex - Market Segmentation, Social Network Analysis, Search result groupings
# Medical imaging, image segmentation, anomaly detection, Recommendation systems etc...

# K means complexity O(n)

# Types of Clustering
  # Centroid Based Clustering
  # Density Based Clustering
  # Distribution Based Clustering
  # Hierarchical Clustering
  
# Steps to cluster data:
  # Prepare data
  # Create similarity Metric
  # Run Clustering Algorithm
  # Interpret results and adjust clustering
  
# Prepare Data:
  # Normalizing Data: well-suited for processing common distributions like Gaussian. 
  # also when data set lacks enough data to create quantiles
  
  # Log Transform: when dataset conforms power law distribution that clumps data at the low end
  # We process a power-law distribution by using log transform
  
  # Quantiles: When data does not conform Gaussian or power-law distribution. General approach
  # that applies to any data distribution
  # Divide the data into intervals where each interval contains equal number of examples. These
  # example boundaries are called quantiles
  # Steps to convert data into quantiles:
    # Decide the number of intervals
    # Define intervals such that each interval has equal number of examples
    # Replace each example by the index of the interval it falls in
    # Bring the indexes to same range as other feature data by scaling the index values to [0,1]
    
  # Quantiles are the best default choice to transform data. Rule of thumb: To create n
  # quantiles, we should have at least 10*n examples. 
  
# Create similarity metric:
  # Manual Similarity Measure:
  # When features are numeric, can combine them into a single number.
  # When features are categorical - Single Valued (univalent), Multi Value (multivalent). 
  # Single valued are easier, when it is Multi Valued then use ratio of common values aka
  # Jaccard similarity
  # To calculate the overall similarity between pairs by combining per-feature similarity
  # using root mean squared error.

  # Supervised Similarity Measure:
  # Instead of comparing manually-combined feature data, we can reduce the feature data
  # to representations called embeddings and then compare the embeddings.
  # Embeddings are generated by training a supervised deep neural network (DNN) on the
  # feature data itself. 
  # Embeddings map the feature data to a vector in embedding space
  
  # When to use manual vs supervised measures:
    # Eliminate redundant information in correlated features:
      # In manual, we need to separately investigate correlations between features, No.
      # In supervised, DNN eliminates redudant information, Yes.
      
    # Provide insight into calculated similaries:
      # In manual ,Yes. 
      # In supervised, embeddings cannot be deciphered, No.
      
    # Suitable for small datasets with few features:
      # In manual, designing a manual measure with few features is easy.
      # In supervised, small datasets do not provide enough training data for DNN.
      
    # Suitable for large datasets with many features:
      # In manual, manually eliminating redundant information from multiple features
      # and then combining them is very difficult, No.
      # In supervised, the DNN automatically eliminates redundant information and combines
      # features, yes.
      
    # Steps for Supervised similarity measure:
      # Input feature data
      # Choose DNN: auto encoder, predictor
      # Extract Embeddings
      # Choose Measurement: Dot Product, Cosine, Euclidean distance
      
    # Choose DNN based on Training Labels:
      # Reduce the feature data to embeddings by training a DNN that uses the same feature
      # data both as input and as the labels
      # Example, use age, bmi, glucose to predict those features themselves
      # In order to use the feature daat to predict the same feature data, the DNN is forced
      # to reduce the input feature data to embeddings. Use these embeddings to calculate similarity.
      # DNN that learns embeddings of input data by predicting the input data itself is called
      # autoencoder. Because an autoencoder's hidden layers are smaller than the input and output
      # layers, the autoencoder is forced to learn a compressed representation of the input feature
      # data. Once the DNN is trained, we extract the embeddings from the last hidden layer to calculate
      # similarity.
      
      # Autoencoder is simplest choice to generate embeddings. 
      # Autoencoder is not optimal choice when certain features could be more important than others in determining similarity.
      # Assume age is more important than bmi for stroke prediction. In such cases, use only
      # the important feature as the training label for the DNN. Since this DNN predicts a specific
      # input feature instead of predicting all input features, it is called a predictor DNN
      
      # Guidelines to Choose a feature as the label:
      # Prefer numeric features to categorical features as labels because loss is easier to 
      # calculate and interpret for numeric features
      # Do not use categorical features with cardinality <= 100 as labels. If we do, DNN will not
      # be forced to reduct input data to embeddings because a DNN can easily predict low-cardinality
      # categorical labels.
      # Remove the feature that we use as the label from the input to DNN else DNN will
      # perfectly predict the output
      
      # Depending on the choice of labels, DNN is either autoencoder DNN or predictor DNN
      
    # Loss Function for DNN
      # Calculate the loss for every output of the DNN.
        # For numeric, use mean squared error
        # For univalent categorical, use log loss
        # For multivalent categorical, use softmax cross entropy loss
      # Calculate total loss by summing the loss for every output.
      
    # Note to ensure that each feature contributes proportionally to the loss. Ex: if we 
    # convert color data to RGB values, then we have three outputs. Summing the loss for
    # three output means weighting three times heavily than other features. Instead, multiply by 1/3
    
    # Using DNN in an Online System:
      # Online ML system has continuous stream of new input data. We need to train DNN on 
      # new data. However, if we retrain the DNN from scratch, then our embeddings will be 
      # different because DNNs are initialized with random weights. Instead, we warm-start
      # the DNN with the existing weights and then update the DNN with new data
    
    
    # Generating Embeddings:
    
      # Imagine we have a housing dataset: Price, Size, Postal Code, Number of Bedrooms, Type of House, Garage, Colors
      
      # Now Pre process the data
        # Price - Poisson Distribution - Quantize and scale to [0,1]
        # Size - Poisson Distribution - Quantize and scale to [0,1]
        # Postal Code - Categorical - Convert to Longitude and Latitude, Quantize and scale to [0,1]
        # Number of bedrooms - Integer - Clip Outliers and scale to [0,1]
        # Type of House - Catgorical - Convert to One-Hot Encoding
        # Garage - 0 or 1 - Leave as is
        # Colors - Categorical - Convert to RGB values and process as numeric data
        
      # Now choose a predictor ot auto encoder:
        # To generate embeddings, we can choose either autoencoder or a predictor. Default
        # choice is autoencoder. We choose predictor instead if specific features in our dataset
        # deetermine similarity. We will look at both cases
        
        # Train a predictor:
          # We need to choose the features as training labels for DNN that are important in
          # determining similarity between examples. Assume price is most important in determining similarity between houses
          
          # Choose a price as the training label, and remove it from the input feature data to
          # the DNN. Train the DNN by using all other features as input data. For training, the 
          # loss function is simply the MSE between predicted and actual price
          
        # Train a Autoencoder:
          # Ensure the hidden layers of the autoencoder are smaller than the input and output layers
          # Calculate the loss for each output 
          # Create the loss function by summing the losses for each output. Ensure the weightings
          # Train the DNN
          
      # Now Extracting Embeddings from the DNN:
        # After training the DNN, whether predictor or autoencoder, extract the embeddings
        # for an example from the DNN. Extract the embedding by using feature data of the 
        # example as input, and read the outputs of the final hidden layer. These outputs
        # form the embedding vector. 
        
      # Now we Measure Similarity from Embeddings
        # We have three choices: Euclidean distance, Cosine and Dot Product. When we have
        # examples that appear frequently in the training set ( ex: popular youtube videos)
        # then they tend to have embedding vectors with large lengths. If we want to capture
        # popularity then we choose dot product. However, the risk is that popular examples
        # may skew the similarity metric. To balance this skew, we can raise the length to
        # an exponent a > 1 to calcuate the dot product
        
    # How to check quality of clustering?
      # Commonly used metrics: Cluster Cardinality, Cluster Magnitude, Performance of Downstream system
      
    # Cluster Cardinality: is the number of examples per cluster. Plot the cluster cardinality,
    # for all clusters and investigate for outliers. Cluster vs Number of Points in Cluster
    
    # Cluster Magnitude: is the sum of distances from all examples to the centroid of the 
    # cluster.Check how the magnitude varies across clusters and investiage anomalies.
    # Cluster vs total point-to-centroid distance
    
    # Magnitude vs Cardinality: Higher cluster cardinality tends to have higer magnitude. Clusters
    # are anomalous when cardinality does not correlate with magnitude relative to other 
    # clusters. Plot Magnitude aganist Cardinality for inspection
    
    
    # Performance of the Similarity Measure:
    
    # Simple Check: identify pairs of examples that are more or less similar than other pairs
    # then calculate the similarity measure for each pair of examples and ensure that the 
    # similarity measure for more similar examples is higher than the similiarity measure for
    # less similar examples.
    
    
    # Optimum Number of Clusters: Plot the Number of Clusters against Total point-to-centroid
    # distance. When loss becomes marginal with increasing k, we can approximate the k value.

import os
import pandas as pd
import numpy as np
os.chdir("C:/Users/kkhar/OneDrive/Desktop/K-Means-Clustering")

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

xls = pd.ExcelFile("retail.xlsx")
sheet_names = xls.sheet_names
num_sheets = len(sheet_names)
print("Sheet Names:", sheet_names)
print("Number of Sheets:", num_sheets)

data1 = pd.read_excel("retail.xlsx", sheet_name = 0)
data2 = pd.read_excel("retail.xlsx", sheet_name = 1)

data1.columns
data2.columns

data1.info()
data2.info()

stacked_df = pd.concat([data1, data2], ignore_index = True)

null_mask = stacked_df.isnull().any(axis = 1)
null_rows = stacked_df[null_mask]
print(null_rows)

stacked = stacked_df.dropna(subset = ['Customer ID'])
stacked.isnull().sum()

# Removing duplicate entries from a table
import pandasql as psql

query = """
    SELECT Invoice, StockCode, Description, Quantity, InvoiceDate, Price, [Customer ID], Country
    FROM stacked
    GROUP BY Invoice, StockCode, Description, Quantity, InvoiceDate, Price, [Customer ID], Country
"""
stacked = psql.sqldf(query)

def starts_with_letter(string):
  string = str(string)
  return string[0].isalpha()

stacked = stacked[~stacked['Invoice'].apply(starts_with_letter)]
stacked.describe()

stacked = stacked[~(stacked['Price'] < 0)]

numeric_columns = stacked.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0

negative_mask.any()
stacked = stacked[~(stacked['Quantity'] < 0)]
stacked.columns.to_list()



# RFM Analysis of Customers 

# Recency: If a customer made a purchase within last 3 months then we 
# call them recent customers
stacked.head(n=2)

final_data = stacked.copy()
final_data.describe()

dataset_max = final_data['InvoiceDate'].max()
dataset_max = pd.to_datetime(dataset_max)  
customer_max = final_data.groupby('Customer ID', as_index = False)['InvoiceDate'].max()
customer_max.columns = ['Customer ID', 'Latest_Invoice_Date']
customer_max['Latest_Invoice_Date'] = pd.to_datetime(customer_max['Latest_Invoice_Date'])
customer_max['Recency'] = customer_max.Latest_Invoice_Date.apply(lambda x: (dataset_max - x).days) 
customer_max = customer_max.drop(['Latest_Invoice_Date'], axis = 1)

# Frequency, How Frequent Customers are making purchases
Invoice_Count = final_data.groupby(['Customer ID'], as_index = False).agg({'Invoice' : lambda x:len(x)})
Invoice_Count.columns = ['Customer ID', 'Frequency']

# Monetary, How much did Customers Spend
final_data['Sales'] = final_data['Price']*final_data['Quantity']
Total_Sales = final_data.groupby('Customer ID')['Sales'].sum().reset_index()
Total_Sales.columns = ['Customer ID', 'Monetary']
Total_Sales = Total_Sales.reset_index(drop = True)
customer_max = customer_max.reset_index(drop = True)
Invoice_Count = Invoice_Count.reset_index(drop = True)
merge1 = pd.merge(customer_max, Invoice_Count, on = 'Customer ID')
df = pd.merge(merge1, Total_Sales, on = 'Customer ID')
df.head(n=2)

Total_Sales[Total_Sales['Customer ID']==18102]


def calculate_rfm_scores(dataframe, r_col, f_col, m_col):
  #dataframe.sort_values(by=[r_col, f_col, m_col], inplace=True)  # Sort the DataFrame by columns
  r_quantiles = dataframe[r_col].quantile([0.2, 0.4, 0.6, 0.8])
  f_quantiles = dataframe[f_col].quantile([0.2, 0.4, 0.6, 0.8])
  m_quantiles = dataframe[m_col].quantile([0.2, 0.4, 0.6, 0.8])

  def R_score(recency):
    if recency <= r_quantiles.iloc[0]:
      return 1
    elif recency > r_quantiles.iloc[0] and recency <= r_quantiles.iloc[1]:
      return 2
    elif recency > r_quantiles.iloc[1] and recency <= r_quantiles.iloc[2]:
      return 3
    elif recency > r_quantiles.iloc[2] and recency <= r_quantiles.iloc[3]:
      return 4
    else:
      return 5

  def F_score(frequency):
    if frequency <= f_quantiles.iloc[0]:
      return 1
    elif frequency > f_quantiles.iloc[0] and frequency <= f_quantiles.iloc[1]:
      return 2
    elif frequency > f_quantiles.iloc[1] and frequency <= f_quantiles.iloc[2]:
      return 3
    elif frequency > f_quantiles.iloc[2] and frequency <= f_quantiles.iloc[3]:
      return 4
    else:
      return 5

  def M_score(monetary):
    if monetary <= m_quantiles.iloc[0]:
      return 5
    elif monetary > m_quantiles.iloc[0] and monetary <= m_quantiles.iloc[1]:
      return 4
    elif monetary > m_quantiles.iloc[1] and monetary <= m_quantiles.iloc[2]:
      return 3
    elif monetary > m_quantiles.iloc[2] and monetary <= m_quantiles.iloc[3]:
      return 2
    else:
      return 1

  dataframe['R'] = dataframe[r_col].apply(R_score)
  dataframe['F'] = dataframe[f_col].apply(F_score)
  dataframe['M'] = dataframe[m_col].apply(M_score)

  return dataframe[['R', 'F', 'M']]

rfm_scores = calculate_rfm_scores(df, 'Recency', 'Frequency', 'Monetary')
df.head(n=2)    

#df.tail()
#df[df['Customer ID'] == 18286]
#18286, #18287
# 1,4,2 and 4,2,1

#Customer ID Recency Frequency Monetary R F M Country

# do not merge, need to process Country in Lat Long - exclude for now

df['RFM'] = df['R'].astype(str)+df['F'].astype(str) + df['M'].astype(str)
df.head(n=2)

import seaborn as sns
import matplotlib.pyplot as plt
sns.displot(df['Recency'])
plt.show()

sns.displot(df['Frequency'])
plt.show()

sns.displot(df['Monetary'])
plt.show()


sns.displot(np.log(df['Recency']))
plt.show()

sns.displot(np.log(df['Frequency']))
plt.show()

sns.displot(np.log(df['Monetary']+1))
plt.show()

df['Frequency'].describe()
df['Recency'].describe()
df['Monetary'].describe()

# Monetary Log Transfrom and scale to 0 and 1

df['log_M'] = np.log(df['Monetary']+1)

sns.displot((df['log_M']))
plt.show()

# x_scaled = x - x_min/x_max - x_min
def minMaxScaler(numcol):
  minx = np.min(numcol)
  maxx = np.max(numcol)
  numcol = (numcol - minx) / (maxx - minx)
  return numcol


df['scaled_Monetary'] = minMaxScaler(df['log_M'])

sns.displot(df['scaled_Monetary'])
plt.show()

df.info()



df['log_F'] = np.log(df['Frequency']+1)
sns.displot(df['log_F'])
plt.show()


df['scaled_Frequency'] = minMaxScaler(df['log_F'])
sns.displot(df['scaled_Frequency'])
plt.show()


df['log_R'] = np.log(df['Recency']+1)
sns.displot(np.log(df['Recency']+1))
plt.show()


df['scaled_Recency'] = minMaxScaler(df['log_R'])
sns.displot(df['scaled_Recency'])
plt.show()


df['scaled_RFM'] = minMaxScaler(df['RFM'].astype(int))
sns.displot(df['scaled_RFM'])
plt.show()

fig, ax = plt.subplots()
sns.kdeplot(df['scaled_Recency'], label='Recency', ax=ax)
sns.kdeplot(df['scaled_Frequency'], label='Frequency', ax=ax)
sns.kdeplot(df['scaled_Monetary'], label='Monetary', ax=ax)
ax.legend()
plt.show()

plt.subplots()
sns.kdeplot((df['RFM'].astype(int)))
plt.show()

def rfm_score_to_label(score):
  rfm_mapping = {
    555: "Champions",
    554: "Champions",
    553: "LoyalCustomers",
    552: "PotentialLoyalist",
    551: "PotentialLoyalist",
    545: "Champions",
    544: "Champions",
    543: "LoyalCustomers",
    542: "PotentialLoyalist",
    541: "PotentialLoyalist",
    535: "PotentialLoyalist",
    534: "PotentialLoyalist",
    533: "PotentialLoyalist",
    532: "PotentialLoyalist",
    531: "PotentialLoyalist",
    525: "Promising",
    524: "Promising",
    523: "Promising",
    522: "RecentCustomers",
    521: "RecentCustomers",
    515: "Promising",
    514: "Promising",
    513: "Promising",
    512: "RecentCustomers",
    511: "RecentCustomers",
    455: "Champions",
    454: "Champions",
    453: "LoyalCustomers",
    452: "PotentialLoyalist",
    451: "PotentialLoyalist",
    445: "Champions",
    444: "Champions",
    443: "LoyalCustomers",
    442: "PotentialLoyalist",
    441: "PotentialLoyalist",
    435: "PotentialLoyalist",
    434: "PotentialLoyalist",
    433: "PotentialLoyalist",
    432: "PotentialLoyalist",
    431: "PotentialLoyalist",
    425: "Promising",
    424: "Promising",
    423: "Promising",
    422: "RecentCustomers",
    421: "RecentCustomers",
    415: "Promising",
    414: "Promising",
    413: "Promising",
    412: "RecentCustomers",
    411: "RecentCustomers",
    355: "LoyalCustomers",
    354: "LoyalCustomers",
    353: "PotentialLoyalist",
    352: "PotentialLoyalist",
    351: "PotentialLoyalist",
    345: "LoyalCustomers",
    344: "LoyalCustomers",
    343: "NeedAttention",
    342: "NeedAttention",
    341: "NeedAttention",
    335: "NeedAttention",
    334: "NeedAttention",
    333: "NeedAttention",
    332: "NeedAttention",
    331: "AboutToSleep",
    325: "NeedAttention",
    324: "NeedAttention",
    323: "NeedAttention",
    322: "AboutToSleep",
    321: "AboutToSleep",
    315: "NeedAttention",
    314: "NeedAttention",
    313: "AboutToSleep",
    312: "AboutToSleep",
    311: "AboutToSleep",
    255: "AtRisk",
    254: "AtRisk",
    253: "AtRisk",
    252: "AboutToSleep",
    251: "AboutToSleep",
    245: "AtRisk",
    244: "AtRisk",
    243: "AtRisk",
    242: "AboutToSleep",
    241: "AboutToSleep",
    235: "AtRisk",
    234: "AtRisk",
    233: "AtRisk",
    232: "AboutToSleep",
    231: "AboutToSleep",
    225: "AtRisk",
    224: "AtRisk",
    223: "Hibernating",
    222: "Hibernating",
    221: "AboutToSleep",
    215: "AtRisk",
    214: "AtRisk",
    213: "Hibernating",
    212: "Hibernating",
    211: "Hibernating",
    155: "CannotLoseThem",
    154: "CannotLoseThem",
    153: "Hibernating",
    152: "Hibernating",
    151: "Lost",
    145: "CannotLoseThem",
    144: "CannotLoseThem",
    143: "Hibernating",
    142: "Hibernating",
    141: "Lost",
    135: "CannotLoseThem",
    134: "CannotLoseThem",
    133: "Hibernating",
    132: "Hibernating",
    131: "Lost",
    125: "CannotLoseThem",
    124: "CannotLoseThem",
    123: "Hibernating",
    122: "Lost",
    121: "Lost",
    115: "CannotLoseThem",
    114: "CannotLoseThem",
    113: "Lost",
    112: "Lost",
    111: "Lost",
  }
  return rfm_mapping.get(score, "Unknown")

backup = df.copy()
df = backup

df.head(n=2)
df['label'] = df['RFM'].astype(int).apply(rfm_score_to_label)
df['label'].unique()

df = pd.get_dummies(df, columns=['label'])
df[df.columns[df.columns.str.startswith('label_')]] = df[df.columns[df.columns.str.startswith('label_')]].astype(int)
df.columns.to_list()
df.head(n=2)



from scipy.stats import skew
df.head(n=2)

skew(df['Recency']**(0.32)) # Power transformation
skew(df['Frequency'])
skew(df['Monetary'])

import seaborn as sns
import matplotlib.pyplot as plt
sns.displot(df['Recency']**(1/3), kde = True)
plt.show()

sns.displot(np.log(df['Frequency']), kde = True)
plt.show()

sns.displot(np.log(df['Monetary']), kde = True)
plt.show()



import diptest

def bimodal_test(data, feature):
  dip_statistic, p_value = diptest.diptest(data[feature])
  alpha = 0.05
  if p_value < alpha:
    print(f"Reject the null hypothesis for {feature}: Data is not unimodal (potentially bimodal or multimodal).")
  else:
    print(f"Fail to reject the null hypothesis for {feature}: Data appears unimodal.")
  print(f"Dip Statistic for {feature}: {dip_statistic}")
  print(f"P-value for {feature}: {p_value}")
  
bimodal_test(data = df, feature = 'scaled_Frequency')  
bimodal_test(data = df, feature = 'scaled_Recency')  
bimodal_test(data = df, feature = 'scaled_Monetary')  

from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=1)  
gmm.fit((df['scaled_Frequency']).to_numpy().reshape(-1, 1))
thresholds = gmm.means_
thresholds = pd.DataFrame(thresholds).squeeze()
thresholds

df[df['Frequency']==1]

#normality test
from scipy.stats import anderson, shapiro, kstest

result = anderson(df['Recency']**0.32)
print('Anderson-Darling Test Statistic:', result.statistic)
print('Critical Values:', result.critical_values)

stat, p = shapiro(df['Recency']**0.32)
print('Shapiro-Wilk Test Statistic:', stat)
print('p-value:', p)

stat, p = kstest(df['Recency']**0.32, 'norm')
print('Kolmogorov-Smirnov Test Statistic:', stat)
print('p-value:', p)


from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=1)  
gmm.fit((df['Recency']**0.32).to_numpy().reshape(-1, 1))
thresholds = gmm.means_
thresholds = pd.DataFrame(thresholds).squeeze()
thresholds

df['Recency_bimodal'] = ((df['Recency']**0.32) > thresholds.squeeze()).astype(int)
df.head(n=2)


cols = [ 'Customer ID', 'scaled_Monetary', 'scaled_Frequency', 'scaled_Recency', 'label_AboutToSleep', 'label_AtRisk', 'label_CannotLoseThem', 'label_Champions', 'label_Hibernating', 'label_Lost', 'label_LoyalCustomers', 'label_NeedAttention', 'label_PotentialLoyalist', 'label_Promising', 'label_RecentCustomers']
cluster_data = df[cols].astype(float)
cluster_data.head()

cluster_data_b = cluster_data
cluster_data = cluster_data_b

cor_df = cluster_data
correlation_matrix = cor_df.corr()
plt.figure(figsize = (10,6))
heatmap = sns.heatmap(correlation_matrix, annot = True, fmt = "0.2f", cmap = "coolwarm_r", annot_kws={"size": 12})
heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, horizontalalignment="right", fontsize = 14)
heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation = 0, horizontalalignment = "right", fontsize = 14)
plt.title("Correlation Heatmap of Numerical Features", fontsize = 16)
plt.savefig('correlation_plot.jpg', format = 'jpg', dpi = 300, bbox_inches = 'tight')
plt.show()

from sklearn.decomposition import PCA
n_components = 0.95
pca = PCA(n_components=n_components)
principal_components = pca.fit_transform(cluster_data)

# Create a DataFrame to store the principal components
principal_df = pd.DataFrame(data=principal_components)
principal_df.head(n=2)

explained_variance_ratio = pca.explained_variance_ratio_
explained_variance_ratio.sum()

correlation_matrix = principal_df.corr()
plt.figure(figsize = (10,6))
heatmap = sns.heatmap(correlation_matrix, annot = True, fmt = "0.2f", cmap = "coolwarm_r", annot_kws={"size": 12})
heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, horizontalalignment="right", fontsize = 14)
heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation = 0, horizontalalignment = "right", fontsize = 14)
plt.title("Correlation Heatmap of Numerical Features", fontsize = 16)
plt.savefig('correlation_plot.jpg', format = 'jpg', dpi = 300, bbox_inches = 'tight')
plt.show()


principal_df.head(n=2)

cluster_data.head(n=2)






# final data dictionary

cluster_data_dict = cluster_data.set_index('Customer ID').apply(lambda x: x.values.tolist(), axis=1).to_dict()

first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
  print(f'{key}: {value}')
  
  
class Centroid:
  def __init__(self, location):
    self.location = location
    self.closest_users = set()
    
import random
k = 3
initial_centroids_customers = random.sample(sorted(list(cluster_data_dict.keys())), k) # randomly select k customers as initial centroid
initial_centroids_customers

centroids = {f'cluster{ik}Centroids': cluster_data_dict[initial_centroids_customers[ik]] for ik in range(k)} # get the feature values of randomly selected customer which is our initial centroid
centroids

clusters = {f'cluster{ik}CustomerID': [] for ik in range(k)} # initialize empty list to store Customer IDs that gets assigned to each cluster
clusters

num_features_per_user = 14
#distance = {f'Centroid{ik}distance': {u: sum([centroids[f'cluster{ik}Centroids'][j] - cluster_data_dict[u][j] for j in range(num_features_per_user)]) for u in cluster_data_dict} for ik in range(k)}

# Calculate the distance from centroid to each datapoint
distance = {} # Initialize an empty dictionary to store distances

for ik in range(k): # loop over customer ID that is initialized as centroid
  centroid_distances = {} # Create an empty dictionary to store distances from this customer ID to all other customer ID
  
  for u in cluster_data_dict: # loop over all customer ID
    total_distance = 0 # Initialize the total distance for this customer ID to be zero
    
    # calculate the distance or dissimilarity or the difference between each feature of the customer ID and centroid
    for j in range(num_features_per_user): # Loop over each feature dimension
      total_distance += centroids[f'cluster{ik}Centroids'][j] - cluster_data_dict[u][j] # Calculate the distance along each feature dimension and add it to tota_distance
    
    centroid_distances[u] = total_distance
    
  distance[f'Centroid{ik}distance'] = centroid_distances
