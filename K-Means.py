# K- Means Clustering

# Use Cases ex - Market Segmentation, Social Network Analysis, Search result groupings
# Medical imaging, image segmentation, anomaly detection, Recommendation systems etc...

# K means complexity O(n)

# Types of Clustering
  # Centroid Based Clustering
  # Density Based Clustering
  # Distribution Based Clustering
  # Hierarchical Clustering
  
# Steps to cluster data:
  # Prepare data
  # Create similarity Metric
  # Run Clustering Algorithm
  # Interpret results and adjust clustering
  
# Prepare Data:
  # Normalizing Data: well-suited for processing common distributions like Gaussian. 
  # also when data set lacks enough data to create quantiles
  
  # Log Transform: when dataset conforms power law distribution that clumps data at the low end
  # We process a power-law distribution by using log transform
  
  # Quantiles: When data does not conform Gaussian or power-law distribution. General approach
  # that applies to any data distribution
  # Divide the data into intervals where each interval contains equal number of examples. These
  # example boundaries are called quantiles
  # Steps to convert data into quantiles:
    # Decide the number of intervals
    # Define intervals such that each interval has equal number of examples
    # Replace each example by the index of the interval it falls in
    # Bring the indexes to same range as other feature data by scaling the index values to [0,1]
    
  # Quantiles are the best default choice to transform data. Rule of thumb: To create n
  # quantiles, we should have at least 10*n examples. 
  
# Create similarity metric:
  # Manual Similarity Measure:
  # When features are numeric, can combine them into a single number.
  # When features are categorical - Single Valued (univalent), Multi Value (multivalent). 
  # Single valued are easier, when it is Multi Valued then use ratio of common values aka
  # Jaccard similarity
  # To calculate the overall similarity between pairs by combining per-feature similarity
  # using root mean squared error.

  # Supervised Similarity Measure:
  # Instead of comparing manually-combined feature data, we can reduce the feature data
  # to representations called embeddings and then compare the embeddings.
  # Embeddings are generated by training a supervised deep neural network (DNN) on the
  # feature data itself. 
  # Embeddings map the feature data to a vector in embedding space
  
  # When to use manual vs supervised measures:
    # Eliminate redundant information in correlated features:
      # In manual, we need to separately investigate correlations between features, No.
      # In supervised, DNN eliminates redudant information, Yes.
      
    # Provide insight into calculated similaries:
      # In manual ,Yes. 
      # In supervised, embeddings cannot be deciphered, No.
      
    # Suitable for small datasets with few features:
      # In manual, designing a manual measure with few features is easy.
      # In supervised, small datasets do not provide enough training data for DNN.
      
    # Suitable for large datasets with many features:
      # In manual, manually eliminating redundant information from multiple features
      # and then combining them is very difficult, No.
      # In supervised, the DNN automatically eliminates redundant information and combines
      # features, yes.
      
    # Steps for Supervised similarity measure:
      # Input feature data
      # Choose DNN: auto encoder, predictor
      # Extract Embeddings
      # Choose Measurement: Dot Product, Cosine, Euclidean distance
      
    # Choose DNN based on Training Labels:
      # Reduce the feature data to embeddings by training a DNN that uses the same feature
      # data both as input and as the labels
      # Example, use age, bmi, glucose to predict those features themselves
      # In order to use the feature daat to predict the same feature data, the DNN is forced
      # to reduce the input feature data to embeddings. Use these embeddings to calculate similarity.
      # DNN that learns embeddings of input data by predicting the input data itself is called
      # autoencoder. Because an autoencoder's hidden layers are smaller than the input and output
      # layers, the autoencoder is forced to learn a compressed representation of the input feature
      # data. Once the DNN is trained, we extract the embeddings from the last hidden layer to calculate
      # similarity.
      
      # Autoencoder is simplest choice to generate embeddings. 
      # Autoencoder is not optimal choice when certain features could be more important than others in determining similarity.
      # Assume age is more important than bmi for stroke prediction. In such cases, use only
      # the important feature as the training label for the DNN. Since this DNN predicts a specific
      # input feature instead of predicting all input features, it is called a predictor DNN
      
      # Guidelines to Choose a feature as the label:
      # Prefer numeric features to categorical features as labels because loss is easier to 
      # calculate and interpret for numeric features
      # Do not use categorical features with cardinality <= 100 as labels. If we do, DNN will not
      # be forced to reduct input data to embeddings because a DNN can easily predict low-cardinality
      # categorical labels.
      # Remove the feature that we use as the label from the input to DNN else DNN will
      # perfectly predict the output
      
      # Depending on the choice of labels, DNN is either autoencoder DNN or predictor DNN
      
    # Loss Function for DNN
      # Calculate the loss for every output of the DNN.
        # For numeric, use mean squared error
        # For univalent categorical, use log loss
        # For multivalent categorical, use softmax cross entropy loss
      # Calculate total loss by summing the loss for every output.
      
    # Note to ensure that each feature contributes proportionally to the loss. Ex: if we 
    # convert color data to RGB values, then we have three outputs. Summing the loss for
    # three output means weighting three times heavily than other features. Instead, multiply by 1/3
    
    # Using DNN in an Online System:
      # Online ML system has continuous stream of new input data. We need to train DNN on 
      # new data. However, if we retrain the DNN from scratch, then our embeddings will be 
      # different because DNNs are initialized with random weights. Instead, we warm-start
      # the DNN with the existing weights and then update the DNN with new data
    
    
    # Generating Embeddings:
    
      # Imagine we have a housing dataset: Price, Size, Postal Code, Number of Bedrooms, Type of House, Garage, Colors
      
      # Now Pre process the data
        # Price - Poisson Distribution - Quantize and scale to [0,1]
        # Size - Poisson Distribution - Quantize and scale to [0,1]
        # Postal Code - Categorical - Convert to Longitude and Latitude, Quantize and scale to [0,1]
        # Number of bedrooms - Integer - Clip Outliers and scale to [0,1]
        # Type of House - Catgorical - Convert to One-Hot Encoding
        # Garage - 0 or 1 - Leave as is
        # Colors - Categorical - Convert to RGB values and process as numeric data
        
      # Now choose a predictor ot auto encoder:
        # To generate embeddings, we can choose either autoencoder or a predictor. Default
        # choice is autoencoder. We choose predictor instead if specific features in our dataset
        # deetermine similarity. We will look at both cases
        
        # Train a predictor:
          # We need to choose the features as training labels for DNN that are important in
          # determining similarity between examples. Assume price is most important in determining similarity between houses
          
          # Choose a price as the training label, and remove it from the input feature data to
          # the DNN. Train the DNN by using all other features as input data. For training, the 
          # loss function is simply the MSE between predicted and actual price
          
        # Train a Autoencoder:
          # Ensure the hidden layers of the autoencoder are smaller than the input and output layers
          # Calculate the loss for each output 
          # Create the loss function by summing the losses for each output. Ensure the weightings
          # Train the DNN
          
      # Now Extracting Embeddings from the DNN:
        # After training the DNN, whether predictor or autoencoder, extract the embeddings
        # for an example from the DNN. Extract the embedding by using feature data of the 
        # example as input, and read the outputs of the final hidden layer. These outputs
        # form the embedding vector. 
        
      # Now we Measure Similarity from Embeddings
        # We have three choices: Euclidean distance, Cosine and Dot Product. When we have
        # examples that appear frequently in the training set ( ex: popular youtube videos)
        # then they tend to have embedding vectors with large lengths. If we want to capture
        # popularity then we choose dot product. However, the risk is that popular examples
        # may skew the similarity metric. To balance this skew, we can raise the length to
        # an exponent a > 1 to calcuate the dot product
        
    # How to check quality of clustering?
      # Commonly used metrics: Cluster Cardinality, Cluster Magnitude, Performance of Downstream system
      
    # Cluster Cardinality: is the number of examples per cluster. Plot the cluster cardinality,
    # for all clusters and investigate for outliers. Cluster vs Number of Points in Cluster
    
    # Cluster Magnitude: is the sum of distances from all examples to the centroid of the 
    # cluster.Check how the magnitude varies across clusters and investiage anomalies.
    # Cluster vs total point-to-centroid distance
    
    # Magnitude vs Cardinality: Higher cluster cardinality tends to have higer magnitude. Clusters
    # are anomalous when cardinality does not correlate with magnitude relative to other 
    # clusters. Plot Magnitude aganist Cardinality for inspection
    
    
    # Performance of the Similarity Measure:
    
    # Simple Check: identify pairs of examples that are more or less similar than other pairs
    # then calculate the similarity measure for each pair of examples and ensure that the 
    # similarity measure for more similar examples is higher than the similiarity measure for
    # less similar examples.
    
    
    # Optimum Number of Clusters: Plot the Number of Clusters against Total point-to-centroid
    # distance. When loss becomes marginal with increasing k, we can approximate the k value.

import os
import pandas as pd
import numpy as np
os.chdir("C:/Users/kkhar/OneDrive/Desktop/K-Means-Clustering")

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

xls = pd.ExcelFile("retail.xlsx")
sheet_names = xls.sheet_names
num_sheets = len(sheet_names)
print("Sheet Names:", sheet_names)
print("Number of Sheets:", num_sheets)

data1 = pd.read_excel("retail.xlsx", sheet_name = 0)
data2 = pd.read_excel("retail.xlsx", sheet_name = 1)

data1.columns
data2.columns

data1.info()
data2.info()

stacked_df = pd.concat([data1, data2], ignore_index = True)

null_mask = stacked_df.isnull().any(axis = 1)
null_rows = stacked_df[null_mask]
print(null_rows)

stacked = stacked_df.dropna(subset = ['Customer ID'])
stacked.isnull().sum()

# Removing duplicate entries from a table

import pandasql as psql

query = """
    SELECT Invoice, StockCode, Description, Quantity, InvoiceDate, Price, [Customer ID], Country
    FROM stacked
    GROUP BY Invoice, StockCode, Description, Quantity, InvoiceDate, Price, [Customer ID], Country
"""
stacked = psql.sqldf(query)

def starts_with_letter(string):
  string = str(string)
  return string[0].isalpha()

stacked = stacked[~stacked['Invoice'].apply(starts_with_letter)]
stacked.describe()

stacked = stacked[~(stacked['Price'] <= 0)]

numeric_columns = stacked.select_dtypes(include=['number'])
negative_mask = numeric_columns < 0

negative_mask.any()
stacked = stacked[~(stacked['Quantity'] < 0)]
stacked.columns.to_list()



# RFM Analysis of Customers 

# Recency: If a customer made a purchase within last 3 months then we 
# call them recent customers

stacked.head(n=2)

final_data = stacked.copy()
final_data.describe()

dataset_max = final_data['InvoiceDate'].max()
dataset_max = pd.to_datetime(dataset_max)  
customer_max = final_data.groupby('Customer ID', as_index = False)['InvoiceDate'].max()
customer_max.columns = ['Customer ID', 'Latest_Invoice_Date']
customer_max['Latest_Invoice_Date'] = pd.to_datetime(customer_max['Latest_Invoice_Date'])
customer_max['Recency'] = customer_max.Latest_Invoice_Date.apply(lambda x: (dataset_max - x).days) 
customer_max = customer_max.drop(['Latest_Invoice_Date'], axis = 1)

# Frequency, How Frequent Customers are making purchases
Invoice_Count = final_data.groupby(['Customer ID'], as_index = False).agg({'Invoice' : lambda x:len(x)})
Invoice_Count.columns = ['Customer ID', 'Frequency']

# Monetary, How much did Customers Spend
final_data['Sales'] = final_data['Price']*final_data['Quantity']
Total_Sales = final_data.groupby('Customer ID')['Sales'].sum().reset_index()
Total_Sales.columns = ['Customer ID', 'Monetary']
Total_Sales = Total_Sales.reset_index(drop = True)
customer_max = customer_max.reset_index(drop = True)
Invoice_Count = Invoice_Count.reset_index(drop = True)
merge1 = pd.merge(customer_max, Invoice_Count, on = 'Customer ID')
df = pd.merge(merge1, Total_Sales, on = 'Customer ID')
df.head(n=2)

Total_Sales[Total_Sales['Customer ID']==18102]


def calculate_rfm_scores(dataframe, r_col, f_col, m_col):
  #dataframe.sort_values(by=[r_col, f_col, m_col], inplace=True)  # Sort the DataFrame by columns
  r_quantiles = dataframe[r_col].quantile([0.2, 0.4, 0.6, 0.8])
  f_quantiles = dataframe[f_col].quantile([0.2, 0.4, 0.6, 0.8])
  m_quantiles = dataframe[m_col].quantile([0.2, 0.4, 0.6, 0.8])

  def R_score(recency):
    if recency <= r_quantiles.iloc[0]:
      return 1
    elif recency > r_quantiles.iloc[0] and recency <= r_quantiles.iloc[1]:
      return 2
    elif recency > r_quantiles.iloc[1] and recency <= r_quantiles.iloc[2]:
      return 3
    elif recency > r_quantiles.iloc[2] and recency <= r_quantiles.iloc[3]:
      return 4
    else:
      return 5

  def F_score(frequency):
    if frequency <= f_quantiles.iloc[0]:
      return 5
    elif frequency > f_quantiles.iloc[0] and frequency <= f_quantiles.iloc[1]:
      return 4
    elif frequency > f_quantiles.iloc[1] and frequency <= f_quantiles.iloc[2]:
      return 3
    elif frequency > f_quantiles.iloc[2] and frequency <= f_quantiles.iloc[3]:
      return 2
    else:
      return 1

  def M_score(monetary):
    if monetary <= m_quantiles.iloc[0]:
      return 5
    elif monetary > m_quantiles.iloc[0] and monetary <= m_quantiles.iloc[1]:
      return 4
    elif monetary > m_quantiles.iloc[1] and monetary <= m_quantiles.iloc[2]:
      return 3
    elif monetary > m_quantiles.iloc[2] and monetary <= m_quantiles.iloc[3]:
      return 2
    else:
      return 1

  dataframe['R'] = dataframe[r_col].apply(R_score)
  dataframe['F'] = dataframe[f_col].apply(F_score)
  dataframe['M'] = dataframe[m_col].apply(M_score)

  return dataframe[['R', 'F', 'M']]

rfm_scores = calculate_rfm_scores(df, 'Recency', 'Frequency', 'Monetary')
df.head(n=2)    

#df.tail()
#df[df['Customer ID'] == 18286]
#18286, #18287
# 1,4,2 and 4,2,1

#Customer ID Recency Frequency Monetary R F M Country

# do not merge, need to process Country in Lat Long - exclude for now

df['RFM'] = df['R'].astype(str)+df['F'].astype(str) + df['M'].astype(str)
df.head(n=2)

import seaborn as sns
import matplotlib.pyplot as plt

sns.displot(df['Recency']+1)
plt.show()

sns.displot(df['Frequency']+1)
plt.xlim(0,1000)
plt.show()

sns.displot(df['Monetary']+1)
plt.xlim(0,10000)
plt.show()

# Check the skewness of data
from scipy.stats import skew

skew((df['Recency']+1)**0.32) 
skew(df['Frequency']+1)
skew(df['Monetary']+1)

# We can see that Frequency and Monetary is highly skewed to the right. We
# need to make it look more normal and one  way is to use log transformation in these
# type of cases. None of the variables are symmetric

# Check kurtosis of data
from scipy.stats import kurtosis

kurtosis(df['Recency']+1) 
kurtosis(df['Frequency']+1)
kurtosis(df['Monetary']+1)

# The kurtosis value for Recency is negative, indicating that the distribution has
# lighter tails and is flatter than a normal distribution (platykurtic). This
# suggests that extreme values are less likely 
# The kurtosis value for Frequency and Monetary is extremely high, indicating a very
# heavy tailed distribution(leptokurtic). It suggests that there are extreme values
# and the distribution of data has a very high peak. Suggests present of outliers
# or highly skewed distribution.

# The kurtosis for Recency is already quite close to zero suggesting that the
# distribution is not far from normal distribution

# The k-means algorithm is based on the mean of data points within clusters and its
# performance can be affected by the distribution of data. K means makes the assumption
# that clusters are spherical and equally sized, and it tries to minimize the variance
# within clusters which makes this algorithm sensitive to scaling and shape of the 
# clusters.

# Hence, we transform and scale the variables
from scipy.stats import boxcox

df['power_R'] = (df['Recency']+1)**0.32
df['log_F'] = np.log(df['Frequency']+1)
df['log_M'] = np.log(df['Monetary']+1)

fig, ax = plt.subplots()
sns.kdeplot(df['power_R'], label='Power Recency', ax=ax)
sns.kdeplot(df['log_F'], label='Log Frequency', ax=ax)
sns.kdeplot(df['log_M'], label='Log Monetary', ax=ax)
ax.legend()
plt.show()

# x_scaled = x - x_min/x_max - x_min
def minMaxScaler(numcol):
  minx = np.min(numcol)
  maxx = np.max(numcol)
  numcol = (numcol - minx) / (maxx - minx)
  return numcol


# Scaling Recency, Frequency and Monetary Values 
df['scaled_Recency'] = minMaxScaler(df['power_R']) 
df['scaled_Frequency'] = minMaxScaler(df['log_F'])
df['scaled_Monetary']  = minMaxScaler(df['log_M'])
df['scaled_RFM'] = minMaxScaler(df['RFM'].astype(int))
df['scaled_R'] = minMaxScaler(df['R'])
df['scaled_F'] = minMaxScaler(df['F'])
df['scaled_M'] = minMaxScaler(df['M'])

# I will create a new variable recency_bimodal to represent two distinct peaks in recency variable

from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=1)  
gmm.fit((df['Recency']**0.32).to_numpy().reshape(-1, 1))
thresholds = gmm.means_
thresholds = pd.DataFrame(thresholds).squeeze()
print(thresholds)
df['Recency_bimodal'] = ((df['Recency']**0.32) > thresholds.squeeze()).astype(int)


# Now I will map the RFM scores to its segments

def rfm_score_to_label(score):
  rfm_mapping = {
    555: "Champions",
    554: "Champions",
    553: "LoyalCustomers",
    552: "PotentialLoyalist",
    551: "PotentialLoyalist",
    545: "Champions",
    544: "Champions",
    543: "LoyalCustomers",
    542: "PotentialLoyalist",
    541: "PotentialLoyalist",
    535: "PotentialLoyalist",
    534: "PotentialLoyalist",
    533: "PotentialLoyalist",
    532: "PotentialLoyalist",
    531: "PotentialLoyalist",
    525: "Promising",
    524: "Promising",
    523: "Promising",
    522: "RecentCustomers",
    521: "RecentCustomers",
    515: "Promising",
    514: "Promising",
    513: "Promising",
    512: "RecentCustomers",
    511: "RecentCustomers",
    455: "Champions",
    454: "Champions",
    453: "LoyalCustomers",
    452: "PotentialLoyalist",
    451: "PotentialLoyalist",
    445: "Champions",
    444: "Champions",
    443: "LoyalCustomers",
    442: "PotentialLoyalist",
    441: "PotentialLoyalist",
    435: "PotentialLoyalist",
    434: "PotentialLoyalist",
    433: "PotentialLoyalist",
    432: "PotentialLoyalist",
    431: "PotentialLoyalist",
    425: "Promising",
    424: "Promising",
    423: "Promising",
    422: "RecentCustomers",
    421: "RecentCustomers",
    415: "Promising",
    414: "Promising",
    413: "Promising",
    412: "RecentCustomers",
    411: "RecentCustomers",
    355: "LoyalCustomers",
    354: "LoyalCustomers",
    353: "PotentialLoyalist",
    352: "PotentialLoyalist",
    351: "PotentialLoyalist",
    345: "LoyalCustomers",
    344: "LoyalCustomers",
    343: "NeedAttention",
    342: "NeedAttention",
    341: "NeedAttention",
    335: "NeedAttention",
    334: "NeedAttention",
    333: "NeedAttention",
    332: "NeedAttention",
    331: "AboutToSleep",
    325: "NeedAttention",
    324: "NeedAttention",
    323: "NeedAttention",
    322: "AboutToSleep",
    321: "AboutToSleep",
    315: "NeedAttention",
    314: "NeedAttention",
    313: "AboutToSleep",
    312: "AboutToSleep",
    311: "AboutToSleep",
    255: "AtRisk",
    254: "AtRisk",
    253: "AtRisk",
    252: "AboutToSleep",
    251: "AboutToSleep",
    245: "AtRisk",
    244: "AtRisk",
    243: "AtRisk",
    242: "AboutToSleep",
    241: "AboutToSleep",
    235: "AtRisk",
    234: "AtRisk",
    233: "AtRisk",
    232: "AboutToSleep",
    231: "AboutToSleep",
    225: "AtRisk",
    224: "AtRisk",
    223: "Hibernating",
    222: "Hibernating",
    221: "AboutToSleep",
    215: "AtRisk",
    214: "AtRisk",
    213: "Hibernating",
    212: "Hibernating",
    211: "Hibernating",
    155: "CannotLoseThem",
    154: "CannotLoseThem",
    153: "Hibernating",
    152: "Hibernating",
    151: "Lost",
    145: "CannotLoseThem",
    144: "CannotLoseThem",
    143: "Hibernating",
    142: "Hibernating",
    141: "Lost",
    135: "CannotLoseThem",
    134: "CannotLoseThem",
    133: "Hibernating",
    132: "Hibernating",
    131: "Lost",
    125: "CannotLoseThem",
    124: "CannotLoseThem",
    123: "Hibernating",
    122: "Lost",
    121: "Lost",
    115: "CannotLoseThem",
    114: "CannotLoseThem",
    113: "Lost",
    112: "Lost",
    111: "Lost",
  }
  return rfm_mapping.get(score, "Unknown")


backup = df.copy()
df = backup

df.head(n=2)

# Creating a label variable to store all the customer segmentation mappings
df['label'] = df['RFM'].astype(int).apply(rfm_score_to_label)
df['label'].unique()

# Converting customer segmentation mappings to dummy variables
df = pd.get_dummies(df, columns=['label'])
df[df.columns[df.columns.str.startswith('label_')]] = df[df.columns[df.columns.str.startswith('label_')]].astype(int)
df.columns.to_list()
df.head(n=2)
df['R']

# Now we will filter the variables that goes into our k-means model
df.columns.to_list()
cols = [ 'Customer ID', 'scaled_R', 'scaled_F', 'scaled_M', 'scaled_Monetary', 'scaled_Frequency', 'scaled_Recency', 'Recency_bimodal', 'label_AboutToSleep', 'label_AtRisk', 'label_CannotLoseThem', 'label_Champions', 'label_Hibernating', 'label_Lost', 'label_LoyalCustomers', 'label_NeedAttention', 'label_PotentialLoyalist', 'label_Promising', 'label_RecentCustomers']
cluster_data = df[cols].astype(float)
cluster_data.head()
cluster_data.columns.to_list()


cor_df = cluster_data
correlation_matrix = cor_df.corr()
plt.figure(figsize = (10,6))
heatmap = sns.heatmap(correlation_matrix, annot = True, fmt = "0.2f", cmap = "coolwarm_r", annot_kws={"size": 8})
heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, horizontalalignment="right", fontsize = 12)
heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation = 0, horizontalalignment = "right", fontsize = 12)
plt.title("Correlation Heatmap of Features", fontsize = 12)
plt.savefig('correlation_plot.jpg', format = 'jpg', dpi = 300, bbox_inches = 'tight')
plt.show()

# From the correlation plot, we can see that some features are highly correlated, we will capture the
# most important patterns in the data and avoid redundancy. Since the algorithm is sensitive to scale and
# correlation of features, PCA may help us improve the performance of the model.

from sklearn.decomposition import PCA

pca = PCA(n_components = 0.99) # keep 99% variability of data
principal_components = pca.fit_transform(cluster_data.loc[:, ~cluster_data.columns.isin(['Customer ID'])])

# Create a DataFrame to store the principal components
principal_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(principal_components.shape[1])])
principal_df['Customer ID'] = cluster_data['Customer ID']
principal_df.head(n=2)

explained_variance_ratio = pca.explained_variance_ratio_
explained_variance_ratio.sum()


# final data dictionary

cluster_data_dict = principal_df.set_index('Customer ID').apply(lambda x: x.values.tolist(), axis=1).to_dict()

first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
  print(f'{key}: {value}')
  
import random

class Centroid:
  def __init__(self, location):
    self.location = location
    self.closest_users = set()

k = 11 # choose number of clusters, this should be based on some mathematical form (elbow method etc..)
initial_centroids_customers = random.sample(sorted(list(cluster_data_dict.keys())), k) # randomly select k customers as initial centroid

# Initialize centroids using Centroid class
centroids = {f'cluster{ik}Centroids': Centroid(cluster_data_dict[initial_centroids_customers[ik]]) for ik in range(k)} # get the feature values of randomly selected customer which is our initial centroid

# Initialize clusters dictionary
clusters = {f'cluster{ik}CustomerID': [] for ik in range(k)}  # initialize empty list to store Customer IDs that gets assigned to each cluster

num_features_per_user = principal_components.shape[1]

for i in range(10): # iterate 20 times
  distance = {f'Centroid{ik}distance': {} for ik in range(k)} # empty dict to store distances of each customer ID to all centroids
  for ik in range(k): # loop over customer ID that is initialized as centroid
    centroid_distances = {}  # Create an empty dictionary to store distances from this customer ID(centroid) to all other customer ID
    for u in cluster_data_dict: # loop over all customer ID
      # calculate the distance or dissimilarity or the difference between each feature(principal components) of the customer ID and centroid
      total_distance = sum(abs(centroids[f'cluster{ik}Centroids'].location[j] - cluster_data_dict[u][j]) for j in range(num_features_per_user)) # Calculate the manhattan distance along each feature(principal components) dimension of the customer and centroid vectors
      centroid_distances[u] = total_distance
    distance[f'Centroid{ik}distance'] = centroid_distances
  # Once we find the distance from each point to the centroids, we need to assign the points to its closet centroid
  for u in cluster_data_dict: # loop over all customer ID
    distances = [distance[f'Centroid{ik}distance'][u] for ik in range(k)] # list that contains the distances between a specific customer ID and all the centroids
    nearest_centroid_index = distances.index(min(distances)) # get the nearest centroid
    clusters[f'cluster{nearest_centroid_index}CustomerID'].append(u) # assign the customer to the centroid
  # Update centroids based on the mean of the assigned datapoints
  for ik in range(k): # iterate over each cluster index
    if clusters[f'cluster{ik}CustomerID']: # chekc if cluster with index ik has any assigned data points, If empty, no need to update centroid
      mean_values = [sum(cluster_data_dict[u][j] for u in clusters[f'cluster{ik}CustomerID']) / len(clusters[f'cluster{ik}CustomerID']) for j in range(num_features_per_user)] # Compute the mean values across all data points assigned to the current cluster by iterating over each feature dimension(j).
      centroids[f'cluster{ik}Centroids'].location = mean_values  # Update centroid location



