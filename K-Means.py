# K- Means Clustering

# Use Cases ex - Market Segmentation, Social Network Analysis, Search result groupings
# Medical imaging, image segmentation, anomaly detection, Recommendation systems etc...

# K means complexity O(n)

# Types of Clustering
  # Centroid Based Clustering
  # Density Based Clustering
  # Distribution Based Clustering
  # Hierarchical Clustering
  
# Steps to cluster data:
  # Prepare data
  # Create similarity Metric
  # Run Clustering Algorithm
  # Interpret results and adjust clustering
  
# Prepare Data:
  # Normalizing Data: well-suited for processing common distributions like Gaussian. 
  # also when data set lacks enough data to create quantiles
  
  # Log Transform: when dataset conforms power law distribution that clumps data at the low end
  # We process a power-law distribution by using log transform
  
  # Quantiles: When data does not conform Gaussian or power-law distribution. General approach
  # that applies to any data distribution
  # Divide the data into intervals where each interval contains equal number of examples. These
  # example boundaries are called quantiles
  # Steps to convert data into quantiles:
    # Decide the number of intervals
    # Define intervals such that each interval has equal number of examples
    # Replace each example by the index of the interval it falls in
    # Bring the indexes to same range as other feature data by scaling the index values to [0,1]
    
  # Quantiles are the best default choice to transform data. Rule of thumb: To create n
  # quantiles, we should have at least 10*n examples. 
  
# Create similarity metric:
  # Manual Similarity Measure:
  # When features are numeric, can combine them into a single number.
  # When features are categorical - Single Valued (univalent), Multi Value (multivalent). 
  # Single valued are easier, when it is Multi Valued then use ratio of common values aka
  # Jaccard similarity
  # To calculate the overall similarity between pairs by combining per-feature similarity
  # using root mean squared error.

  # Supervised Similarity Measure:
  # Instead of comparing manually-combined feature data, we can reduce the feature data
  # to representations called embeddings and then compare the embeddings.
  # Embeddings are generated by training a supervised deep neural network (DNN) on the
  # feature data itself. 
  # Embeddings map the feature data to a vector in embedding space
  
  # When to use manual vs supervised measures:
    # Eliminate redundant information in correlated features:
      # In manual, we need to separately investigate correlations between features, No.
      # In supervised, DNN eliminates redudant information, Yes.
      
    # Provide insight into calculated similaries:
      # In manual ,Yes. 
      # In supervised, embeddings cannot be deciphered, No.
      
    # Suitable for small datasets with few features:
      # In manual, designing a manual measure with few features is easy.
      # In supervised, small datasets do not provide enough training data for DNN.
      
    # Suitable for large datasets with many features:
      # In manual, manually eliminating redundant information from multiple features
      # and then combining them is very difficult, No.
      # In supervised, the DNN automatically eliminates redundant information and combines
      # features, yes.
      
    # Steps for Supervised similarity measure:
      # Input feature data
      # Choose DNN: auto encoder, predictor
      # Extract Embeddings
      # Choose Measurement: Dot Product, Cosine, Euclidean distance
      
    # Choose DNN based on Training Labels:
      # Reduce the feature data to embeddings by training a DNN that uses the same feature
      # data both as input and as the labels
      # Example, use age, bmi, glucose to predict those features themselves
      # In order to use the feature daat to predict the same feature data, the DNN is forced
      # to reduce the input feature data to embeddings. Use these embeddings to calculate similarity.
      # DNN that learns embeddings of input data by predicting the input data itself is called
      # autoencoder. Because an autoencoder's hidden layers are smaller than the input and output
      # layers, the autoencoder is forced to learn a compressed representation of the input feature
      # data. Once the DNN is trained, we extract the embeddings from the last hidden layer to calculate
      # similarity.
      
      # Autoencoder is simplest choice to generate embeddings. 
      # Autoencoder is not optimal choice when certain features could be more important than others in determining similarity.
      # Assume age is more important than bmi for stroke prediction. In such cases, use only
      # the important feature as the training label for the DNN. Since this DNN predicts a specific
      # input feature instead of predicting all input features, it is called a predictor DNN
      
      # Guidelines to Choose a feature as the label:
      # Prefer numeric features to categorical features as labels because loss is easier to 
      # calculate and interpret for numeric features
      # Do not use categorical features with cardinality <= 100 as labels. If we do, DNN will not
      # be forced to reduct input data to embeddings because a DNN can easily predict low-cardinality
      # categorical labels.
      # Remove the feature that we use as the label from the input to DNN else DNN will
      # perfectly predict the output
      
      # Depending on the choice of labels, DNN is either autoencoder DNN or predictor DNN
      
    # Loss Function for DNN
      # Calculate the loss for every output of the DNN.
        # For numeric, use mean squared error
        # For univalent categorical, use log loss
        # For multivalent categorical, use softmax cross entropy loss
      # Calculate total loss by summing the loss for every output.
      
    # Note to ensure that each feature contributes proportionally to the loss. Ex: if we 
    # convert color data to RGB values, then we have three outputs. Summing the loss for
    # three output means weighting three times heavily than other features. Instead, multiply by 1/3
    
    # Using DNN in an Online System:
      # Online ML system has continuous stream of new input data. We need to train DNN on 
      # new data. However, if we retrain the DNN from scratch, then our embeddings will be 
      # different because DNNs are initialized with random weights. Instead, we warm-start
      # the DNN with the existing weights and then update the DNN with new data
    
    
    # Generating Embeddings:
    
      # Imagine we have a housing dataset: Price, Size, Postal Code, Number of Bedrooms, Type of House, Garage, Colors
      
      # Now Pre process the data
        # Price - Poisson Distribution - Quantize and scale to [0,1]
        # Size - Poisson Distribution - Quantize and scale to [0,1]
        # Postal Code - Categorical - Convert to Longitude and Latitude, Quantize and scale to [0,1]
        # Number of bedrooms - Integer - Clip Outliers and scale to [0,1]
        # Type of House - Catgorical - Convert to One-Hot Encoding
        # Garage - 0 or 1 - Leave as is
        # Colors - Categorical - Convert to RGB values and process as numeric data
        
      # Now choose a predictor ot auto encoder:
        # To generate embeddings, we can choose either autoencoder or a predictor. Default
        # choice is autoencoder. We choose predictor instead if specific features in our dataset
        # deetermine similarity. We will look at both cases
        
        # Train a predictor:
          # We need to choose the features as training labels for DNN that are important in
          # determining similarity between examples. Assume price is most important in determining similarity between houses
          
          # Choose a price as the training label, and remove it from the input feature data to
          # the DNN. Train the DNN by using all other features as input data. For training, the 
          # loss function is simply the MSE between predicted and actual price
          
        # Train a Autoencoder:
          # Ensure the hidden layers of the autoencoder are smaller than the input and output layers
          # Calculate the loss for each output 
          # Create the loss function by summing the losses for each output. Ensure the weightings
          # Train the DNN
          
      # Now Extracting Embeddings from the DNN:
        # After training the DNN, whether predictor or autoencoder, extract the embeddings
        # for an example from the DNN. Extract the embedding by using feature data of the 
        # example as input, and read the outputs of the final hidden layer. These outputs
        # form the embedding vector. 
        
      # Now we Measure Similarity from Embeddings
        # We have three choices: Euclidean distance, Cosine and Dot Product. When we have
        # examples that appear frequently in the training set ( ex: popular youtube videos)
        # then they tend to have embedding vectors with large lengths. If we want to capture
        # popularity then we choose dot product. However, the risk is that popular examples
        # may skew the similarity metric. To balance this skew, we can raise the length to
        # an exponent a > 1 to calcuate the dot product
        
    # How to check quality of clustering?
      # Commonly used metrics: Cluster Cardinality, Cluster Magnitude, Performance of Downstream system
      
    # Cluster Cardinality: is the number of examples per cluster. Plot the cluster cardinality,
    # for all clusters and investigate for outliers. Cluster vs Number of Points in Cluster
    
    # Cluster Magnitude: is the sum of distances from all examples to the centroid of the 
    # cluster.Check how the magnitude varies across clusters and investiage anomalies.
    # Cluster vs total point-to-centroid distance
    
    # Magnitude vs Cardinality: Higher cluster cardinality tends to have higer magnitude. Clusters
    # are anomalous when cardinality does not correlate with magnitude relative to other 
    # clusters. Plot Magnitude aganist Cardinality for inspection
    
    
    # Performance of the Similarity Measure:
    
    # Simple Check: identify pairs of examples that are more or less similar than other pairs
    # then calculate the similarity measure for each pair of examples and ensure that the 
    # similarity measure for more similar examples is higher than the similiarity measure for
    # less similar examples.
    
    
    # Optimum Number of Clusters: Plot the Number of Clusters against Total point-to-centroid
    # distance. When loss becomes marginal with increasing k, we can approximate the k value.

import os
import pandas as pd
import numpy as np
os.chdir("C:/Users/kkhar/OneDrive/Desktop/K-Means-Clustering")

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

def readexcel(excelfile):
  xls = pd.ExcelFile(excelfile)
  sheet_names = xls.sheet_names
  num_sheets = len(sheet_names)
  print("Sheet Names:", sheet_names)
  print("Number of Sheets:", num_sheets)
  
  data = {}
  
  for sheet_index in range(len(sheet_names)):
    data[sheet_index] = pd.read_excel(excelfile, sheet_name = sheet_index)
  
  df = pd.concat(data.values(), ignore_index = True)
  return df


df = readexcel(excelfile = "retail.xlsx")


def starts_with_letter(string):
  string = str(string)
  return string[0].isalpha()


def datacleaning(df):
  print(df.info())
  null_mask = df.isnull().any(axis=1)
  null_rows = df[null_mask]
  print("Count of Null Rows before Removal: ", len(null_rows))
  df = df.dropna()
  print("Count of Nulls in Columns after Removal: ", df.isnull().sum())
  print("Number of Records after Null Removal: ", len(df))
  print("Number of records before removing duplicates: ", df.count())
  df = df.drop_duplicates()
  print("Number of records after removing duplicates: ", df.count())
  print("Summary of Data: ", df.describe())
  print("Removing Invoices that start with letters...")
  df = df[~df['Invoice'].apply(starts_with_letter)]
  print("Number of records after removing invoices starting with letters: ", len(df))
  print("Removing prices that are negative or zero...")
  df = df[~(df['Price'] <= 0)]
  print("Number of records after removing negatively priced products: ", len(df))
  print("Removing Negative Quantities...")
  df = df[~(df['Quantity'] < 0)]
  print("Number of records after removing Negative Quantity products: ", len(df))
  print("Summary of Data:", df.describe())
  return df  
  
df = datacleaning(df=df)
df.head(n=2)

# RFM Analysis of Customers 

# Recency: If a customer made a purchase within last 3 months then we 
# call them recent customers

def get_recency(df):
  final_data = df.copy()
  dataset_max = pd.to_datetime(final_data['InvoiceDate'].max())
  customer_max = final_data.groupby('Customer ID', as_index = False)['InvoiceDate'].max()
  customer_max.columns = ['Customer ID', 'Latest_Invoice_Date']
  customer_max['Latest_Invoice_Date'] = pd.to_datetime(customer_max['Latest_Invoice_Date'])
  customer_max['Recency'] = customer_max.Latest_Invoice_Date.apply(lambda x: (dataset_max - x).days) 
  customer_max = customer_max.drop(['Latest_Invoice_Date'], axis = 1)
  return customer_max

get_recency(df=df)  
  
def get_frequency(df):
  final_data = df.copy()
  Item_Count = final_data.groupby(['Customer ID'], as_index = False).agg({'Invoice': lambda x: len(x)})
  Item_Count.columns = ['Customer ID', 'Frequency']
  return Item_Count

get_frequency(df=df)


def get_monetary(df):
  final_data = df.copy()
  final_data['Sales'] = final_data['Price']*final_data['Quantity']
  Total_Sales = final_data.groupby('Customer ID')['Sales'].sum().reset_index()
  Total_Sales.columns = ['Customer ID', 'Monetary']
  return Total_Sales

get_monetary(df = df)


def mergedata(df):
  recency = get_recency(df = df)
  recency = recency.reset_index(drop = True)
  frequency = get_frequency(df = df)
  frequency = frequency.reset_index(drop = True)
  monetary = get_monetary(df = df)
  monetary = monetary.reset_index(drop = True)
  merge1 = pd.merge(recency, frequency, on = 'Customer ID')
  merge2 = pd.merge(merge1, monetary, on = 'Customer ID')
  return merge2
  
df = mergedata(df = df)  
df.head(n=2)


def calculate_rfm_scores(dataframe, r_col, f_col, m_col):
  #dataframe.sort_values(by=[r_col, f_col, m_col], inplace=True)  # Sort the DataFrame by columns
  r_quantiles = dataframe[r_col].quantile([0.2, 0.4, 0.6, 0.8])
  f_quantiles = dataframe[f_col].quantile([0.2, 0.4, 0.6, 0.8])
  m_quantiles = dataframe[m_col].quantile([0.2, 0.4, 0.6, 0.8])

  def R_score(recency):
    if recency <= r_quantiles.iloc[0]:
      return 5
    elif recency > r_quantiles.iloc[0] and recency <= r_quantiles.iloc[1]:
      return 4
    elif recency > r_quantiles.iloc[1] and recency <= r_quantiles.iloc[2]:
      return 3
    elif recency > r_quantiles.iloc[2] and recency <= r_quantiles.iloc[3]:
      return 2
    else:
      return 1

  def F_score(frequency):
    if frequency <= f_quantiles.iloc[0]:
      return 1
    elif frequency > f_quantiles.iloc[0] and frequency <= f_quantiles.iloc[1]:
      return 2
    elif frequency > f_quantiles.iloc[1] and frequency <= f_quantiles.iloc[2]:
      return 3
    elif frequency > f_quantiles.iloc[2] and frequency <= f_quantiles.iloc[3]:
      return 4
    else:
      return 5

  def M_score(monetary):
    if monetary <= m_quantiles.iloc[0]:
      return 1
    elif monetary > m_quantiles.iloc[0] and monetary <= m_quantiles.iloc[1]:
      return 2
    elif monetary > m_quantiles.iloc[1] and monetary <= m_quantiles.iloc[2]:
      return 3
    elif monetary > m_quantiles.iloc[2] and monetary <= m_quantiles.iloc[3]:
      return 4
    else:
      return 5

  dataframe['R'] = dataframe[r_col].apply(R_score)
  dataframe['F'] = dataframe[f_col].apply(F_score)
  dataframe['M'] = dataframe[m_col].apply(M_score)

  return dataframe[['R', 'F', 'M']]

rfm_scores = calculate_rfm_scores(df, 'Recency', 'Frequency', 'Monetary')
df.head(n=2)    


df['RFM'] = df['R'].astype(str)+df['F'].astype(str) + df['M'].astype(str)
df.head(n=2)

import seaborn as sns
import matplotlib.pyplot as plt

def plot_combined_distribution(data, columns, xlims=None):
  num_plots = len(columns)
  fig, axes = plt.subplots(nrows=1, ncols=num_plots, figsize=(15, 5))

  for i, column in enumerate(columns):
    sns.histplot(data[column], ax=axes[i])
    axes[i].set_title(f'Distribution of {column}')
    if xlims and xlims[i]:
      axes[i].set_xlim(xlims[i])

  plt.tight_layout()
  plt.savefig('correlation_plot.jpg', format = 'jpg', dpi = 300, bbox_inches = 'tight')
  plt.show()

columns_to_plot = ['Recency', 'Frequency', 'Monetary']
xlims = [(None, None), (0, 900), (0, 10000)]

plot_combined_distribution(df, columns_to_plot, xlims)


# Check the skewness of data
from scipy.stats import skew

def check_skew(df, columns):
  skewness = df[columns].skew()
  print("Skewness: ")
  return skewness

check_skew(df, columns = ['Recency', 'Frequency', 'Monetary'])

def check_kurtosis(df, columns):
  skewness = df[columns].kurtosis()
  print("Kurtosis: ")
  return skewness

check_kurtosis(df, columns = ['Recency', 'Frequency', 'Monetary'])

# We can see that Frequency and Monetary is highly skewed to the right. We
# need to make it look more normal and one  way is to use log transformation in these
# type of cases. None of the variables are symmetric

# The kurtosis value for Recency is negative, indicating that the distribution has
# lighter tails and is flatter than a normal distribution (platykurtic). This
# suggests that extreme values are less likely 
# The kurtosis value for Frequency and Monetary is extremely high, indicating a very
# heavy tailed distribution(leptokurtic). It suggests that there are extreme values
# and the distribution of data has a very high peak. Suggests present of outliers
# or highly skewed distribution.

# The kurtosis for Recency is already quite close to zero suggesting that the
# distribution is not far from normal distribution

# The k-means algorithm is based on the mean of data points within clusters and its
# performance can be affected by the distribution of data. K means makes the assumption
# that clusters are spherical and equally sized, and it tries to minimize the variance
# within clusters which makes this algorithm sensitive to scaling and shape of the 
# clusters.

# Hence, we transform and scale the variables

def power_transform(df, columns, power):
  for col in columns:
    df[f'power_{col}'] = (df[columns]+1)**power
  return df 

df = power_transform(df=df, columns = ['Recency'], power = 0.32)
df.head(n=2)

def log_transform(df, columns):
  for col in columns:
    df[f'log_{col}'] = np.log(df[col] + 1)
  return df

df = log_transform(df, columns = ['Frequency', 'Monetary'])
df.head(n=2)


def plot_transformed_kde(df, transformed_cols):
  fig, ax = plt.subplots()

  for col in transformed_cols:
    sns.kdeplot(df[col], label=f'{col}', ax=ax)

  ax.legend()
  plt.xlabel('')
  plt.savefig('correlation_plot.jpg', format = 'jpg', dpi = 300, bbox_inches = 'tight')
  plt.show()

plot_transformed_kde(df, ['power_Recency', 'log_Frequency', 'log_Monetary'])

# We look at boxplot of above transformed variables

def create_boxplot_multi_variables(df, variable_list):
  plt.clf()
  plt.boxplot([df[var] for var in variable_list], labels=variable_list)
  plt.ylabel('Values')
  plt.title('Box Plot of Variables')
  plt.show()


create_boxplot_multi_variables(df, ['power_Recency', 'log_Frequency', 'log_Monetary'])


def replace_outliers_with_bounds_multi_variables(df, variable_list, percentile_low=25, percentile_high=75):
  for var in variable_list:
    data_series = df[var]
    
    Q1 = np.percentile(data_series, percentile_low)
    Q3 = np.percentile(data_series, percentile_high)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df[var] = np.where((df[var] < lower_bound), lower_bound, df[var])
    df[var] = np.where((df[var] > upper_bound), upper_bound, df[var])

  return df

replace_outliers_with_bounds_multi_variables(df, ['log_Frequency', 'power_Recency', 'log_Monetary'])
# We can see that power transforming the Recency exhibits bimodal density. We will
# apply the GMM model to separate them and scale each one afterwards
# I will create a new variable recency_bimodal to represent two distinct peaks in recency variable using
# gaussian mixture model

# Continuous: If "number of days" refers to a continuous quantity, such as the time elapsed between two events, it is considered continuous. For example, the time duration between two timestamps (measured with high precision) can be treated as a continuous variable.

# Discrete: On the other hand, if "number of days" is used to represent a count or a number of whole days (e.g., the number of days until an event occurs), then it is discrete. Discrete variables take on distinct, separate values and do not have values between them.
from sklearn.mixture import GaussianMixture

def separate_recency(df, columns, components, randomstate):
  gmm = GaussianMixture(n_components=components, random_state=randomstate)
  for cols in columns:
    df[f'bimodal_{cols}'] = gmm.fit_predict(pd.to_numeric(df[columns[0]]).to_numpy().reshape(-1, 1))
  return df

df = separate_recency(df=df, columns=['Recency'], components=2, randomstate=13)
df.head(n=2)

# x_scaled = x - x_min/x_max - x_min
def minMaxScaler(numcol):
  minx = np.min(numcol)
  maxx = np.max(numcol)
  scaled_col = (numcol - minx) / (maxx - minx)
  return scaled_col


# Scaling Recency, Frequency and Monetary Values 
df['scaled_Recency'] = df.groupby('bimodal_Recency')['power_Recency'].transform(minMaxScaler)
df['scaled_Frequency'] = minMaxScaler(df['log_Frequency'])
df['scaled_Monetary']  = minMaxScaler(df['log_Monetary'])


check_skew(df, columns = ['Recency', 'Frequency', 'Monetary'])
check_skew(df, columns = ['power_Recency', 'log_Frequency', 'log_Monetary'])
check_skew(df, columns = ['scaled_Recency', 'scaled_Frequency', 'scaled_Monetary'])

check_kurtosis(df, columns = ['Recency', 'Frequency', 'Monetary'])
check_kurtosis(df, columns = ['power_Recency', 'log_Frequency', 'log_Monetary'])
check_kurtosis(df, columns = ['scaled_Recency', 'scaled_Frequency', 'scaled_Monetary'])


#Skewness:

#All the skewness values are relatively close to zero, with scaled_Recency being the most negatively skewed (-0.25), scaled_Frequency being close to zero (-0.03), and scaled_Monetary being slightly positively skewed (0.27). Generally, skewness values within the range of -0.5 to 0.5 are considered acceptable for assuming normality.
#Kurtosis:

#The kurtosis values are also within a reasonable range. scaled_Recency and scaled_Frequency both have negative kurtosis values, indicating slightly platykurtic distributions, but the magnitudes are not extreme. scaled_Monetary has a positive kurtosis value, indicating a slightly leptokurtic distribution, but again, the magnitude is not highly pronounced.

plot_transformed_kde(df, ['power_Recency', 'log_Frequency', 'log_Monetary'])
plot_transformed_kde(df, ['scaled_Recency', 'scaled_Frequency', 'scaled_Monetary'])


df.head(n=2)

df['scaled_RFM'] = minMaxScaler(df['RFM'].astype(int))
df['scaled_R'] = minMaxScaler(df['R'])
df['scaled_F'] = minMaxScaler(df['F'])
df['scaled_M'] = minMaxScaler(df['M'])


# Now I will map the RFM scores to its segments

def rfm_score_to_label(score):
  rfm_mapping = {
    555: "Champions",
    554: "Champions",
    553: "LoyalCustomers",
    552: "PotentialLoyalist",
    551: "PotentialLoyalist",
    545: "Champions",
    544: "Champions",
    543: "LoyalCustomers",
    542: "PotentialLoyalist",
    541: "PotentialLoyalist",
    535: "PotentialLoyalist",
    534: "PotentialLoyalist",
    533: "PotentialLoyalist",
    532: "PotentialLoyalist",
    531: "PotentialLoyalist",
    525: "Promising",
    524: "Promising",
    523: "Promising",
    522: "RecentCustomers",
    521: "RecentCustomers",
    515: "Promising",
    514: "Promising",
    513: "Promising",
    512: "RecentCustomers",
    511: "RecentCustomers",
    455: "Champions",
    454: "Champions",
    453: "LoyalCustomers",
    452: "PotentialLoyalist",
    451: "PotentialLoyalist",
    445: "Champions",
    444: "Champions",
    443: "LoyalCustomers",
    442: "PotentialLoyalist",
    441: "PotentialLoyalist",
    435: "PotentialLoyalist",
    434: "PotentialLoyalist",
    433: "PotentialLoyalist",
    432: "PotentialLoyalist",
    431: "PotentialLoyalist",
    425: "Promising",
    424: "Promising",
    423: "Promising",
    422: "RecentCustomers",
    421: "RecentCustomers",
    415: "Promising",
    414: "Promising",
    413: "Promising",
    412: "RecentCustomers",
    411: "RecentCustomers",
    355: "LoyalCustomers",
    354: "LoyalCustomers",
    353: "PotentialLoyalist",
    352: "PotentialLoyalist",
    351: "PotentialLoyalist",
    345: "LoyalCustomers",
    344: "LoyalCustomers",
    343: "NeedAttention",
    342: "NeedAttention",
    341: "NeedAttention",
    335: "NeedAttention",
    334: "NeedAttention",
    333: "NeedAttention",
    332: "NeedAttention",
    331: "AboutToSleep",
    325: "NeedAttention",
    324: "NeedAttention",
    323: "NeedAttention",
    322: "AboutToSleep",
    321: "AboutToSleep",
    315: "NeedAttention",
    314: "NeedAttention",
    313: "AboutToSleep",
    312: "AboutToSleep",
    311: "AboutToSleep",
    255: "AtRisk",
    254: "AtRisk",
    253: "AtRisk",
    252: "AboutToSleep",
    251: "AboutToSleep",
    245: "AtRisk",
    244: "AtRisk",
    243: "AtRisk",
    242: "AboutToSleep",
    241: "AboutToSleep",
    235: "AtRisk",
    234: "AtRisk",
    233: "AtRisk",
    232: "AboutToSleep",
    231: "AboutToSleep",
    225: "AtRisk",
    224: "AtRisk",
    223: "Hibernating",
    222: "Hibernating",
    221: "AboutToSleep",
    215: "AtRisk",
    214: "AtRisk",
    213: "Hibernating",
    212: "Hibernating",
    211: "Hibernating",
    155: "CannotLoseThem",
    154: "CannotLoseThem",
    153: "Hibernating",
    152: "Hibernating",
    151: "Lost",
    145: "CannotLoseThem",
    144: "CannotLoseThem",
    143: "Hibernating",
    142: "Hibernating",
    141: "Lost",
    135: "CannotLoseThem",
    134: "CannotLoseThem",
    133: "Hibernating",
    132: "Hibernating",
    131: "Lost",
    125: "CannotLoseThem",
    124: "CannotLoseThem",
    123: "Hibernating",
    122: "Lost",
    121: "Lost",
    115: "CannotLoseThem",
    114: "CannotLoseThem",
    113: "Lost",
    112: "Lost",
    111: "Lost",
  }
  return rfm_mapping.get(score, "Unknown")



backup = df.copy()
df = backup

df.head(n=2)

df['segment'] = df['RFM'].astype(int).apply(rfm_score_to_label)
df['check_segment'] = df['RFM'].astype(int).apply(rfm_score_to_label)


df = pd.get_dummies(df, columns=['segment'])
df[df.columns[df.columns.str.startswith('segment_')]] = df[df.columns[df.columns.str.startswith('segment_')]].astype(int)
df.columns.to_list()
df.head(n=2)


cols = ['Customer ID', 'scaled_RFM', 'bimodal_Recency', 'scaled_Recency', 'scaled_Frequency', 'scaled_Monetary',  'segment_AboutToSleep', 'segment_AtRisk', 'segment_CannotLoseThem', 'segment_Champions', 'segment_Hibernating', 'segment_Lost', 'segment_LoyalCustomers', 'segment_NeedAttention', 'segment_PotentialLoyalist', 'segment_Promising', 'segment_RecentCustomers']
cluster_data = df[cols].astype(float)

cor_df = cluster_data
correlation_matrix = cor_df.corr()
plt.figure(figsize = (10,6))
heatmap = sns.heatmap(correlation_matrix, annot = True, fmt = "0.2f", cmap = "coolwarm_r", annot_kws={"size": 8})
heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, horizontalalignment="right", fontsize = 12)
heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation = 0, horizontalalignment = "right", fontsize = 12)
plt.title("Correlation Heatmap of Features", fontsize = 12)
plt.savefig('correlation_plot.jpg', format = 'jpg', dpi = 300, bbox_inches = 'tight')
plt.show()

# From the correlation plot, we can see that some features are highly correlated, we will capture the
# most important patterns in the data and avoid redundancy. Since the algorithm is sensitive to scale and
# correlation of features, PCA may help us improve the performance of the model.

from sklearn.decomposition import PCA

pca = PCA(n_components = 0.90) # keep 90% variability of data
principal_components = pca.fit_transform(cluster_data.loc[:, ~cluster_data.columns.isin(['Customer ID'])])

# Create a DataFrame to store the principal components
principal_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(principal_components.shape[1])])
principal_df['Customer ID'] = cluster_data['Customer ID']
principal_df.head(n=2)

explained_variance_ratio = pca.explained_variance_ratio_
explained_variance_ratio.sum()


# final data dictionary

cluster_data_dict = principal_df.set_index('Customer ID').apply(lambda x: x.values.tolist(), axis=1).to_dict()

first_5_records = {k: cluster_data_dict[k] for k in list(cluster_data_dict)[:5]}
for key, value in first_5_records.items():
  print(f'{key}: {value}')

num_keys = len(cluster_data_dict)
print("Number of keys:", num_keys)
num_values = len(cluster_data_dict.values())
print("Number of values:", num_values) 


# Not needed we have domain knowledge of 11 segments.

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

X = principal_components
k_values = range(1, 21)
inertia = []
for k in k_values:
  kmeans = KMeans(n_clusters=k, random_state=11, n_init='auto')
  kmeans.fit(X)
  inertia.append(kmeans.inertia_)
plt.clf()
plt.plot(k_values, inertia, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.savefig('correlation_plot.jpg', format = 'jpg', dpi = 300, bbox_inches = 'tight')
plt.show()


import random
import copy

# We want to keep track of which CustomerID is assigned to which Centroid
class Centroid: #blueprint to create clusters that represents a central point
  def __init__(self): # If we want to create a new central point, we need to provide its initial location
    self.closest_users = set() # empty set to create track of the Customers who are closet to this central point


# set ensures that a customer can only be assigned to single Cluster
# Ordering does not matter. What matters is the above

# Set the number of clusters
num_clusters = 11

# Select random initial centroids
initial_centroid_customerID = random.sample(list(cluster_data_dict.keys()), num_clusters)

# Initialize centroids
centroids = {f'cluster {ik} Centroids': cluster_data_dict[initial_centroid_customerID[ik]] for ik in range(num_clusters)}

# Get the number of features per user
num_features_per_user = len(list(cluster_data_dict.values())[0])

# Main loop for K-means algorithm

# Initialize centroids randomly
centroids = {f'cluster {ik} Centroids': [random.uniform(0, 1) for _ in range(num_features_per_user)] for ik in range(num_clusters)}

# Maximum number of iterations
max_iterations = 20

# Previous centroids to check for convergence
prev_centroids = copy.deepcopy(centroids)

for iteration in range(max_iterations):
    # Initialize clusters at the beginning of each iteration
    clusters = {f'cluster {ik} CustomerID': [] for ik in range(num_clusters)}

    # Calculate distances
    distances = {f'centroid {ik} distance': {user: sum([(centroids[f'cluster {ik} Centroids'][feature] - cluster_data_dict[user][feature])**2 for feature in range(num_features_per_user)]) for user in cluster_data_dict} for ik in range(num_clusters)}

    # Assign each user to the nearest centroid
    for user in cluster_data_dict:
        temp_distance = [distances[f'centroid {ik} distance'][user] for ik in range(num_clusters)]
        clusters[f'cluster {temp_distance.index(min(temp_distance))} CustomerID'].append(user)

    # Update centroids
    for ik in range(num_clusters):
        mean_value = [0] * num_features_per_user
        cluster_size = len(clusters[f'cluster {ik} CustomerID'])
        if cluster_size != 0:
            for user in clusters[f'cluster {ik} CustomerID']:
                mean_value = [mean_value[feature] + cluster_data_dict[user][feature] for feature in range(num_features_per_user)]
            centroids[f'cluster {ik} Centroids'] = [mean_value[feature] / cluster_size for feature in range(num_features_per_user)]
        else:
            # If a cluster is empty, assign a random point as its centroid
            centroids[f'cluster {ik} Centroids'] = cluster_data_dict[random.choice(list(cluster_data_dict.keys()))]

    # Check for convergence
    if prev_centroids == centroids:
        print(f"Converged at iteration {iteration + 1}")
        break

    # Update previous centroids for the next iteration
    prev_centroids = copy.deepcopy(centroids)

    #print(f"Iteration {iteration + 1}: Clusters = {clusters}")

#print("Final Clusters:", clusters)
#print("Final Centroids:", centroids)
#print(centroids.values())

values_list = clusters.get('cluster 1 CustomerID', [])
num_values = len(values_list)
print("Number of values:", num_values)



import pandas as pd
import plotly.graph_objects as go
import plotly.express as px

def plot_clusters_2d(data_dict, centroids, clusters):
  fig = go.Figure()
  colormap = px.colors.qualitative.Set1
  for i, centroid_key in enumerate(centroids):
    if isinstance(centroids[centroid_key], list):
      centroid_location = centroids[centroid_key]
    else:
      centroid_location = centroids[centroid_key].location
    fig.add_trace(go.Scatter(x=[centroid_location[0]], y=[centroid_location[1]], mode='markers', marker=dict(size=8, color='black', symbol='x'), hoverinfo='text', text=[f'Centroid of {centroid_key}'], name=f'Centroids {i}'))
  for i, cluster_key in enumerate(clusters):
    cluster_points = clusters[cluster_key]
    cluster_data = {k: data_dict[k] for k in cluster_points}
    x = [item[0] for item in cluster_data.values()]
    y = [item[1] for item in cluster_data.values()]
    customer_ids = list(cluster_data.keys())
    hover_text = [ f'Customer ID: {customer_id}<br>Count: {len(cluster_data)}<br>RFM: {df.loc[df["Customer ID"] == customer_id, "RFM"].values[0]}<br>Segment: {df.loc[df["Customer ID"] == customer_id, "check_segment"].values[0]}' for customer_id in customer_ids]
    fig.add_trace(go.Scatter(x=x, y=y, mode='markers', marker=dict(size=5, opacity=0.7, color=colormap[i % len(colormap)]), hoverinfo='text', text=hover_text, name=f'Cluster {i} Points'))
  fig.update_layout(xaxis_title='Principal Component 0', yaxis_title='Principal Component 1', title='K-Means Clustering')
  fig.show()

plot_clusters_2d(cluster_data_dict, centroids, clusters)

df['Cluster Label'] = -1 

for ik in range(num_clusters):
  df.loc[df['Customer ID'].isin(clusters[f'cluster {ik} CustomerID']), 'Cluster Label'] = ik

print(df.head())
result = df.groupby(['Cluster Label', 'check_segment'])['Customer ID'].count().reset_index(name='Count')
print(result)
#result[result['Cluster Label']==1]
#df[df['Cluster Label'] == 1].describe()

sns.displot(df['Recency'])
plt.show()
df.head(n=2)

x = df[df['Recency'] < 320]
x['check_segment'].unique()



